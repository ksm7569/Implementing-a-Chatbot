{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab761d86-dc0b-4f48-9c75-b6b519cfff4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in d:\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (0.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d882f5-a37f-4d3f-b534-9251bdfad101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations  # 미래 버전의 기능을 미리 사용 가능하도록 설정\n",
    "import os  # 운영체제 기능 사용 (파일 관련 등)\n",
    "import sys  # 파이썬 인터프리터 관련 기능 사용\n",
    "import subprocess  # 다른 프로그램을 파이썬에서 실행할 때 사용\n",
    "import math  # 수학적 계산을 위한 라이브러리 (삼각함수, 제곱근 등)\n",
    "import random  # 랜덤 값 생성에 사용 (무작위 수 생성)\n",
    "import re  # 정규표현식 사용 (텍스트 처리에 활용)\n",
    "import inspect  # 객체 검사 및 정보를 얻기 위한 라이브러리\n",
    "from dataclasses import dataclass  # 데이터 클래스를 간편히 생성하기 위한 기능\n",
    "from pathlib import Path  # 파일 경로를 객체지향적으로 다룰 수 있게 해주는 라이브러리\n",
    "from typing import List, Tuple  # 타입 힌트(자료형 명시)용\n",
    "import sentencepiece as spm  # 텍스트를 단어 또는 토큰으로 쪼개주는 라이브러리\n",
    "import pandas as pd  # 데이터를 표 형식으로 다룰 때 사용하는 라이브러리\n",
    "import torch  # 인공지능 모델 학습과 연산에 사용하는 주요 프레임워크\n",
    "import torch.nn as nn  # 신경망(Neural Network)을 만들 때 사용하는 모듈\n",
    "import torch.nn.functional as F  # 신경망용 함수(예: 활성화 함수) 제공\n",
    "from torch.utils.data import DataLoader, Dataset  # 데이터를 묶어서 모델 학습에 넣을 수 있게 준비해주는 도구\n",
    "from sklearn.model_selection import train_test_split  # 데이터를 훈련용과 검증용으로 나누는 함수\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# pack_padded_sequence 함수는 길이가 서로 다른 여러 문장을 한 번에 LSTM 같은 신경망에 입력할 때 사용합니다.\n",
    "# 예를 들어 [\"안녕\", \"안녕하세요\", \"안녕하세요 여러분\"] 문장을 숫자 벡터로 바꿔서 신경망에 넣을 때,\n",
    "# 길이가 달라 빈 공간을 채우는 패딩이 필요합니다.\n",
    "# pack_padded_sequence를 사용하면 신경망이 이 패딩된 공간을 무시하고 실제 길이만큼만 연산하여 효율성을 높입니다.\n",
    "# pad_packed_sequence 함수는 위에서 pack_padded_sequence로 압축된 데이터를 다시 원래의 패딩된 형태로 복구할 때 사용합니다.\n",
    "\n",
    "\n",
    "# 난수 생성 시 결과를 항상 동일하게 유지하기 위한 설정 (재현성 보장)\n",
    "seed = 42\n",
    "random.seed(seed)               # 파이썬 random 모듈의 난수 고정\n",
    "torch.manual_seed(seed)         # 토치의 CPU 난수 고정\n",
    "torch.cuda.manual_seed_all(seed) # 토치의 GPU 난수 고정\n",
    "torch.backends.cudnn.deterministic = True   # 쿠다(CUDA) 연산 시 결정적 결과 보장 (항상 동일한 결과가 나오도록)\n",
    "torch.backends.cudnn.benchmark = False      # 성능 최적화 옵션을 끄고 재현성 보장 (항상 같은 결과 유지)\n",
    "\n",
    "# 사용할 장치 선택 (GPU 사용 가능하면 GPU 사용, 없으면 CPU 사용)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c8e47d4-069f-4163-b0ed-3af492302857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 번에 인공지능 모델이 학습할 데이터의 개수를 설정합니다.\n",
    "# 예를 들어 BATCH_SIZE = 64이면, 한 번의 학습 시 데이터를 64개씩 가져와서 학습합니다.\n",
    "BATCH_SIZE = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0c2165-427c-49e4-99b7-1c54a92efa5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트에서 필요 없는 문자들을 제거하여 깨끗하게 만드는 함수입니다.\n",
    "# 예를 들어, \"안녕하세요!!! 저는 AI입니다.^^;\" 같은 문장에서 특수문자를 모두 제거하면\n",
    "# \"안녕하세요 저는 AI입니다\"로 바뀌게 됩니다.\n",
    "def clean_text(s: str) -> str:\n",
    "    # 한글, 영어, 숫자, 공백을 제외한 모든 특수문자 제거합니다.\n",
    "    # 예: \"안녕!! 잘 지내니?\" → \"안녕 잘 지내니\"\n",
    "    s = re.sub(r\"[^가-힣a-zA-Z0-9 ]\", \"\", s)\n",
    "\n",
    "    # 연속된 여러 개의 공백을 하나의 공백으로 바꾸고, 양 끝의 공백을 없앱니다.\n",
    "    # 예: \"안녕   잘   지내니 \" → \"안녕 잘 지내니\"\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # 정리된 텍스트를 반환합니다.\n",
    "    return s\n",
    "\n",
    "\n",
    "# CSV 파일에서 챗봇 학습을 위한 데이터를 읽어옵니다.\n",
    "# 예를 들어 'ChatbotData.csv'에는 질문(Q)과 대답(A)이 들어있습니다.\n",
    "df = pd.read_csv(\"ChatbotData.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# 불러온 데이터를 화면에 출력하여 제대로 불러왔는지 확인합니다.\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82d153a0-0aec-4ca7-b7a5-d926d89afe74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Q                         A  label\n",
       "0                       12시 땡                하루가 또 가네요.      0\n",
       "1                 1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3             3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                     PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                       ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822              힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질문(Q) 데이터를 모두 문자열로 변환 후 깨끗한 텍스트로 정리합니다.\n",
    "df[\"Q\"] = df[\"Q\"].astype(str).map(clean_text)\n",
    "# df[\"Q\"]는 질문 열을 나타냅니다.\n",
    "# astype(str)는 질문 데이터를 모두 문자열 타입으로 변경합니다. 예: 숫자 123은 문자열 \"123\"으로 변환됩니다.\n",
    "# map(clean_text)는 질문 데이터 각각에 clean_text 함수를 적용해 특수문자와 공백을 정리합니다.\n",
    "# 예를 들어 질문이 \"안녕!!   반가워요~\" 라면,\n",
    "# clean_text를 적용한 후 \"안녕 반가워요\"로 바뀌어 저장됩니다.\n",
    "\n",
    "# df 데이터프레임을 출력해서 질문 데이터가 잘 정리되었는지 화면에서 확인합니다.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b65f913-852a-47a3-bdd0-ac63120c8e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡</td>\n",
       "      <td>하루가 또 가네요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임</td>\n",
       "      <td>훔쳐보는 거 티나나봐요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남</td>\n",
       "      <td>설렜겠어요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Q                        A  label\n",
       "0                       12시 땡                하루가 또 가네요      0\n",
       "1                 1지망 학교 떨어졌어                 위로해 드립니다      0\n",
       "2                3박4일 놀러가고 싶다               여행은 언제나 좋죠      0\n",
       "3             3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠      0\n",
       "4                     PPL 심하네                눈살이 찌푸려지죠      0\n",
       "...                       ...                      ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임        티가 나니까 눈치가 보이는 거죠      2\n",
       "11819           훔쳐보는 것도 눈치 보임             훔쳐보는 거 티나나봐요      2\n",
       "11820              흑기사 해주는 짝남                    설렜겠어요      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까  잘 헤어질 수 있는 사이 여부인 거 같아요      2\n",
       "11822              힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"A\"] = df[\"A\"].astype(str).map(clean_text)\n",
    "# df[\"A\"]는 답변 열을 나타냅니다.\n",
    "# astype(str)는 답변 데이터를 모두 문자열 타입으로 변경합니다. 예: 숫자 456은 문자열 \"456\"으로 변환됩니다.\n",
    "# map(clean_text)는 답변 데이터 각각에 clean_text 함수를 적용해 특수문자와 공백을 정리합니다.\n",
    "# 예를 들어 답변이 \"네!!! 좋아요~\"라면,\n",
    "# clean_text를 적용한 후 \"네 좋아요\"로 바뀌어 저장됩니다.\n",
    "\n",
    "# df 데이터프레임을 출력해서 답변 데이터가 잘 정리되었는지 화면에서 확인합니다.\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f161c680-274f-444c-ac60-d6ea53fd5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래는 특수한 단어(토큰)를 정의하는 부분입니다.\n",
    "PAD, SOS, EOS, UNK = \"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"\n",
    "\n",
    "# <pad>: 문장의 길이가 서로 다를 때, 짧은 문장 뒤에 길이를 맞추기 위해 빈 공간을 채우는 용도로 쓰입니다.\n",
    "# 예를 들어, [\"안녕\", \"안녕하세요\"] 두 문장을 최대 길이(5자)로 맞출 때,\n",
    "# [\"안녕<pad><pad><pad>\", \"안녕하세요\"]와 같이 빈 공간을 채웁니다.\n",
    "\n",
    "# <sos>: 문장의 시작을 나타내는 특수 토큰입니다. \"start of sentence\"의 줄임말입니다.\n",
    "# 예를 들어, \"좋은 하루야\"라는 문장을 표현할 때 [\"<sos> 좋은 하루야\"]처럼 문장 맨 앞에 붙여 시작을 나타냅니다.\n",
    "\n",
    "# <eos>: 문장의 끝을 나타내는 특수 토큰입니다. \"end of sentence\"의 줄임말입니다.\n",
    "# 예를 들어, \"좋은 하루야\"라는 문장을 표현할 때 [\"좋은 하루야 <eos>\"]처럼 문장 맨 뒤에 붙여 끝을 나타냅니다.\n",
    "\n",
    "# <unk>: 학습 데이터에 없는, 처음 보는 단어가 나타났을 때 사용하는 특수 토큰입니다. \"unknown\"의 줄임말입니다.\n",
    "# 예를 들어, 학습 시 \"핸드폰\"이라는 단어를 보지 못했다면, 실제 사용 시 \"핸드폰\"이 나타날 때 \"<unk>\"로 처리합니다.\n",
    "\n",
    "# 센텐스피스(SentencePiece) 모델의 파일명 앞부분을 정의합니다.\n",
    "# 센텐스피스는 문장을 단어보다 작은 토큰으로 나누는 역할을 합니다.\n",
    "SPM_MODEL_PREFIX = \"spm_ko_nopunct\"\n",
    "# 위 변수는 센텐스피스 모델 파일의 앞부분 이름입니다.\n",
    "# 예를 들어 실제 파일은 \"spm_ko_nopunct.model\"과 같은 형태로 생성됩니다.\n",
    "\n",
    "# 센텐스피스 모델 파일의 전체 이름을 정의합니다.\n",
    "SPM_MODEL = f\"{SPM_MODEL_PREFIX}.model\"\n",
    "# f\"{SPM_MODEL_PREFIX}.model\"은 문자열 앞에 정의된 SPM_MODEL_PREFIX를 이용해 자동으로 문자열을 구성합니다.\n",
    "# 예: SPM_MODEL_PREFIX가 \"spm_ko_nopunct\"일 때, 이 변수는 \"spm_ko_nopunct.model\"이 됩니다.\n",
    "\n",
    "# 센텐스피스의 어휘 크기(단어의 수)를 정의합니다.\n",
    "SPM_VOCAB_SIZE = 8000\n",
    "# 위 숫자는 센텐스피스가 생성할 수 있는 서로 다른 토큰(작은 단어조각)의 최대 수입니다.\n",
    "# 숫자가 클수록 다양한 단어를 더 잘 표현할 수 있지만, 너무 크면 학습이 어렵고\n",
    "# 너무 작으면 단어를 충분히 표현하지 못할 수 있습니다.\n",
    "\n",
    "# 센텐스피스가 학습할 때 포함할 문자(글자)의 비율을 정의합니다.\n",
    "CHAR_COVERAGE = 0.9995\n",
    "# 이 값은 문장을 이루는 문자 중 센텐스피스가 학습 데이터에서 학습하려고 하는 문자의 비율입니다.\n",
    "# 예를 들어 0.9995는 전체 문자 중 99.95%의 문자를 학습하려고 한다는 뜻이며,\n",
    "# 나머지 매우 드문 문자는 무시됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f861cf-384a-4372-8580-6735d007f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_spm_model(texts: List[str]):\n",
    "    # sentencepiece 모델을 새로 학습시키는 함수입니다.\n",
    "    # 매개변수 texts: 모델을 학습할 때 사용하는 텍스트 문장들의 리스트입니다.\n",
    "    # 예: [\"안녕하세요.\", \"저는 학생입니다\"]와 같은 형태입니다.\n",
    "\n",
    "    # 기존에 학습되어 있던 센텐스피스 모델, vocab 파일을 삭제합니다.\n",
    "    # 새로 학습된 모델을 만들기 위해 기존 파일을 항상 지우고 다시 생성합니다.\n",
    "    for p in [f\"{SPM_MODEL_PREFIX}.model\", f\"{SPM_MODEL_PREFIX}.vocab\"]:\n",
    "        if Path(p).exists():      # 파일이 존재하면\n",
    "            os.remove(p)          # 파일을 삭제합니다. 예: \"spm_ko_nopunct.model\" 삭제\n",
    "\n",
    "    # 임시로 사용할 텍스트 파일 이름을 정의합니다.\n",
    "    tmp = \"spm_corpus_nopunct.txt\"\n",
    "\n",
    "    # texts 리스트에 있는 모든 텍스트를 깨끗하게 정리한 후 임시 텍스트 파일에 저장합니다.\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        for t in texts:\n",
    "            # t는 texts 리스트의 각 문장에 대해 반복합니다.\n",
    "            # clean_text(t)는 특수문자를 제거하고, replace(\"\\n\", \" \")로 줄바꿈 문자를 공백으로 바꿉니다.\n",
    "            t = clean_text(t).replace(\"\\n\", \" \")\n",
    "            f.write(t + \"\\n\")     # 예: \"안녕\\n잘 지내니?\" → \"안녕 잘 지내니\"로 변경된 후 파일에 저장됩니다.\n",
    "\n",
    "    # SentencePieceTrainer로 sentencepiece 모델을 학습시킵니다.\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=tmp,                       # 위에서 만든 임시 텍스트 파일을 학습 데이터로 사용합니다.\n",
    "        model_prefix=SPM_MODEL_PREFIX,   # 생성될 모델의 파일 이름 앞부분입니다. 예: \"spm_ko_nopunct\"\n",
    "        vocab_size=SPM_VOCAB_SIZE,       # 생성할 단어 조각(토큰)의 최대 개수입니다. 예: 8000개\n",
    "        model_type=\"unigram\",            # 토큰을 만드는 방법을 지정합니다. 여기서는 unigram 방식을 사용합니다.\n",
    "        character_coverage=CHAR_COVERAGE,# 전체 문자 중 몇 %를 포함할지 비율을 설정합니다. 예: 0.9995 (99.95%)\n",
    "        unk_id=0,                        # <unk> (알 수 없는 단어) 토큰을 사용하지 않으므로 0으로 설정합니다.\n",
    "        bos_id=-1,                       # <bos> (begin of sentence, 문장 시작) 토큰을 사용하지 않으므로 -1로 설정합니다.\n",
    "        eos_id=-1,                       # <eos> (end of sentence, 문장 끝) 토큰을 별도로 설정하지 않으므로 -1로 설정합니다.\n",
    "        pad_id=-1,                       # <pad> 토큰을 사용하지 않습니다.\n",
    "        remove_extra_whitespaces=True    # 연속 공백을 하나로 줄입니다. 예: \"안녕   잘   지내\" → \"안녕 잘 지내\"\n",
    "    )\n",
    "\n",
    "    # 학습이 끝난 후 더 이상 필요 없는 임시 텍스트 파일을 삭제합니다.\n",
    "    os.remove(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f68bc72-f2cc-40c4-b206-580621af8377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpmVocab:\n",
    "    # 주어진 텍스트 데이터를 이용해 문장을 숫자로 변환하거나 숫자를 문장으로 바꾸어 주는 클래스입니다.\n",
    "\n",
    "    def __init__(self, texts: List[str], model_file: str = SPM_MODEL):\n",
    "        # 클래스를 초기화 메서드입니다.\n",
    "        # texts: 센텐스피스 모델을 학습시킬 때 사용하는 텍스트 데이터의 리스트입니다.\n",
    "        # 예: [\"안녕하세요.\", \"저는 인공지능입니다.\"]\n",
    "        # model_file: 미리 학습된 센텐스피스 모델 파일 이름입니다. 기본값으로는 SPM_MODEL을 사용합니다.\n",
    "\n",
    "        # 주어진 texts 데이터로 센텐스피스 모델을 학습시킵니다.\n",
    "        ensure_spm_model(texts)\n",
    "\n",
    "        # SentencePieceProcessor 객체를 생성합니다. 이 객체를 이용해 문장-숫자 변환을 합니다.\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "\n",
    "        # 학습된 모델을 로드(불러오기)합니다.\n",
    "        # 로드한 모델을 사용하여 문장을 숫자로 변환하거나 숫자를 문장으로 변환할 수 있습니다.\n",
    "        self.sp.Load(model_file)\n",
    "\n",
    "        # 특수한 용도로 미리 지정된 토큰(단어조각) 4개가 있기 때문에,\n",
    "        # 실제 센텐스피스에서 나오는 숫자 앞에 4를 더해서 충돌을 피합니다.\n",
    "        # 예: 센텐스피스가 만든 토큰 번호가 0이면, 실제 사용 시엔 앞에 4를 더해서 4가 됩니다.\n",
    "        self.offset = 4\n",
    "\n",
    "        # 센텐스피스가 생성한 토큰의 개수(어휘의 크기)입니다.\n",
    "        self.spm_size = self.sp.GetPieceSize()\n",
    "\n",
    "        # 전체 어휘 크기입니다. 센텐스피스 토큰 수에 미리 정해진 특수토큰 4개를 더한 값입니다.\n",
    "        self.size = self.offset + self.spm_size\n",
    "\n",
    "        # 특수 토큰의 번호를 설정합니다.\n",
    "        # pad_idx: 길이 맞추기 위한 빈 칸\n",
    "        # sos_idx: 문장 시작\n",
    "        # eos_idx: 문장 끝\n",
    "        # unk_idx: 알 수 없는 단어\n",
    "        self.pad_idx, self.sos_idx, self.eos_idx, self.unk_idx = 0, 1, 2, 3\n",
    "\n",
    "        def encode(self, text: str, add_sos=True, add_eos=True, max_len: int | None = None) -> List[int]:\n",
    "            # 입력된 텍스트 문장을 숫자 리스트로 바꿔주는 함수입니다.\n",
    "            # text: 숫자로 변환할 텍스트 문장입니다. 예: \"안녕하세요\"\n",
    "            # add_sos: 문장 시작 토큰(<sos>)을 리스트 맨 앞에 추가할지 여부입니다. 기본값 True.\n",
    "            # add_eos: 문장 끝 토큰(<eos>)을 리스트 맨 뒤에 추가할지 여부입니다. 기본값 True.\n",
    "            # max_len: 숫자 리스트의 최대 길이. 초과하면 잘라냅니다.\n",
    "        \n",
    "            # 텍스트를 정리합니다.\n",
    "            text = clean_text(text)\n",
    "        \n",
    "            # 센텐스피스를 이용해 텍스트를 숫자로 변환합니다.\n",
    "            # 각 숫자에 self.offset(4)을 더해 특수 토큰 번호와 충돌하지 않게 합니다.\n",
    "            ids = [i + self.offset for i in self.sp.EncodeAsIds(text)]\n",
    "        \n",
    "            # 최대 길이 제한이 있다면, sos/eos까지 고려한 실제 허용 길이를 계산해서 문장 숫자를 자릅니다.\n",
    "            if max_len is not None:\n",
    "                budget = max_len - (1 if add_sos else 0) - (1 if add_eos else 0)\n",
    "                budget = max(0, budget)\n",
    "                ids = ids[:budget]\n",
    "        \n",
    "            # <sos> 추가\n",
    "            if add_sos:\n",
    "                ids = [self.sos_idx] + ids\n",
    "        \n",
    "            # <eos> 추가\n",
    "            if add_eos:\n",
    "                ids = ids + [self.eos_idx]\n",
    "        \n",
    "            # 최종 숫자 리스트 반환\n",
    "            return ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        # 숫자 리스트를 다시 사람이 읽을 수 있는 텍스트 문장으로 바꿔주는 함수입니다.\n",
    "        # ids: 숫자 리스트입니다. 예: [1, 16, 38, 2]\n",
    "    \n",
    "        # 리스트에서 offset(4)을 뺀 숫자를 센텐스피스 모델을 통해 다시 문자로 바꿉니다.\n",
    "        # 특수 토큰 번호는 무시하고 실제 센텐스피스 토큰 번호만 변환합니다.\n",
    "        toks = [i - self.offset for i in ids if i >= self.offset]\n",
    "    \n",
    "        try:\n",
    "            # 센텐스피스 모델의 DecodeIds를 사용해 숫자 리스트를 텍스트로 변환합니다.\n",
    "            # 예: [12, 34] → \"안녕\"\n",
    "            return self.sp.DecodeIds(toks)\n",
    "        except Exception:\n",
    "            # DecodeIds에서 문제가 생기면 각 숫자를 직접 문자 조각으로 변환해서 이어 붙입니다.\n",
    "            # IdToPiece는 숫자를 작은 문자 조각(piece)으로 바꿔줍니다.\n",
    "            return \"\".join(self.sp.IdToPiece(i) for i in toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc009b54-4fed-4f49-832a-17abcba39bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM] vocab size: 8004 (SPM pieces: 8000)\n"
     ]
    }
   ],
   "source": [
    "class SpmVocab:\n",
    "    # 주어진 텍스트 데이터를 이용해 문장을 숫자로 변환하거나 숫자를 문장으로 바꾸어 주는 클래스입니다.\n",
    "\n",
    "    def __init__(self, texts: List[str], model_file: str = SPM_MODEL):\n",
    "        # 클래스를 초기화 메서드입니다.\n",
    "        # texts: 센텐스피스 모델을 학습시킬 때 사용하는 텍스트 데이터의 리스트입니다.\n",
    "        # 예: [\"안녕하세요.\", \"저는 학생입니다\"]\n",
    "        # model_file: 미리 학습된 센텐스피스 모델 파일 이름입니다. 기본값으로는 SPM_MODEL을 사용합니다.\n",
    "\n",
    "        # 주어진 texts 데이터로 센텐스피스 모델을 학습시킵니다.\n",
    "        ensure_spm_model(texts)\n",
    "\n",
    "        # SentencePieceProcessor 객체를 생성합니다. 이 객체를 이용해 문장-숫자 변환을 합니다.\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "\n",
    "        # 학습된 모델을 로드(불러오기)합니다.\n",
    "        # 로드한 모델을 사용하여 문장을 숫자로 변환하거나 숫자를 문장으로 변환할 수 있습니다.\n",
    "        self.sp.Load(model_file)\n",
    "\n",
    "        # 특수한 용도로 미리 지정된 토큰(단어조각) 4개가 있기 때문에,\n",
    "        # 실제 센텐스피스에서 나오는 숫자 앞에 4를 더해서 충돌을 피합니다.\n",
    "        # 예: 센텐스피스가 만든 토큰 번호가 0이면, 실제 사용 시엔 앞에 4를 더해서 4가 됩니다.\n",
    "        self.offset = 4\n",
    "\n",
    "        # 센텐스피스가 생성한 토큰의 개수(어휘의 크기)입니다.\n",
    "        self.spm_size = self.sp.GetPieceSize()\n",
    "\n",
    "        # 전체 어휘 크기입니다. 센텐스피스 토큰 수에 미리 정해진 특수토큰 4개를 더한 값입니다.\n",
    "        self.size = self.offset + self.spm_size\n",
    "\n",
    "        # 특수 토큰의 번호를 설정합니다.\n",
    "        # pad_idx(0): 길이 맞추기 위한 빈 칸\n",
    "        # sos_idx(1): 문장 시작\n",
    "        # eos_idx(2): 문장 끝\n",
    "        # unk_idx(3): 알 수 없는 단어\n",
    "        self.pad_idx, self.sos_idx, self.eos_idx, self.unk_idx = 0, 1, 2, 3\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        text: str,\n",
    "        add_sos: bool = True,\n",
    "        add_eos: bool = True,\n",
    "        max_len: int | None = None,\n",
    "    ) -> List[int]:\n",
    "        # 입력된 텍스트 문장을 숫자 리스트로 바꿔주는 함수입니다.\n",
    "        # text: 숫자로 변환할 텍스트 문장입니다. 예: \"안녕하세요\"\n",
    "        # add_sos: 문장 시작 토큰(<sos>)을 숫자 리스트 맨 앞에 추가할지 여부입니다. 기본값 True.\n",
    "        # add_eos: 문장 끝 토큰(<eos>)을 숫자 리스트 맨 뒤에 추가할지 여부입니다. 기본값 True.\n",
    "        # max_len: 숫자 리스트의 최대 길이. 지정되면 초과된 부분은 잘라냅니다.\n",
    "\n",
    "        # 텍스트 정리\n",
    "        text = clean_text(text)\n",
    "\n",
    "        # 센텐스피스 토크나이저로 문장을 숫자로 변환하고 offset을 더해 충돌을 방지합니다.\n",
    "        ids = [i + self.offset for i in self.sp.EncodeAsIds(text)]\n",
    "\n",
    "        # 최대 길이가 지정된 경우, sos/eos 추가를 고려한 실제 허용 길이를 계산해 문장을 잘라냅니다.\n",
    "        if max_len is not None:\n",
    "            budget = max_len - (1 if add_sos else 0) - (1 if add_eos else 0)\n",
    "            budget = max(0, budget)\n",
    "            ids = ids[:budget]\n",
    "\n",
    "        # sos 추가\n",
    "        if add_sos:\n",
    "            ids = [self.sos_idx] + ids\n",
    "\n",
    "        # eos 추가\n",
    "        if add_eos:\n",
    "            ids = ids + [self.eos_idx]\n",
    "\n",
    "        # 최종 ID 리스트 반환\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        # 숫자 리스트를 다시 사람이 읽을 수 있는 텍스트 문장으로 바꿔주는 함수입니다.\n",
    "        # ids: 숫자 리스트입니다. 예: [1, 16, 38, 2]\n",
    "\n",
    "        # 리스트에서 offset(4)을 뺀 숫자를 센텐스피스 모델을 통해 다시 문자로 바꿉니다.\n",
    "        # 특수 토큰 번호는 무시하고 실제 센텐스피스 토큰 번호만 변환합니다.\n",
    "        toks = [i - self.offset for i in ids if i >= self.offset]\n",
    "\n",
    "        try:\n",
    "            # 센텐스피스 모델의 DecodeIds를 사용해 숫자 리스트를 텍스트로 변환합니다.\n",
    "            # 예: [12, 34] → \"안녕\"\n",
    "            return self.sp.DecodeIds(toks)\n",
    "        except Exception:\n",
    "            # DecodeIds에서 문제가 생기면(오류 발생 시) 각 숫자를 직접 문자조각으로 변환해서 붙여줍니다.\n",
    "            # IdToPiece는 숫자를 작은 문자 조각으로 바꿔줍니다.\n",
    "            return \"\".join(self.sp.IdToPiece(i) for i in toks)\n",
    "\n",
    "\n",
    "# 센텐스피스 모델을 이용해 vocab(어휘 사전)을 생성합니다.\n",
    "# vocab은 문장을 숫자 형태로 바꾸거나 숫자를 다시 문장으로 변환할 때 사용됩니다.\n",
    "# 예: \"안녕\" → [1, 200, 342, 2]\n",
    "vocab = SpmVocab(texts=list(df[\"Q\"].values) + list(df[\"A\"].values))\n",
    "\n",
    "# vocab의 전체 크기와 센텐스피스 모델에서 생성한 토큰(작은 단어 조각)의 개수를 출력합니다.\n",
    "# 예: \"[SPM] vocab size: 8004 (SPM pieces: 8000)\" 라는 결과가 나오면,\n",
    "# vocab 전체 크기는 8004개이며 그중 실제 토큰은 8000개라는 뜻입니다.\n",
    "print(f\"[SPM] vocab size: {vocab.size} (SPM pieces: {vocab.spm_size})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9afd62cb-4b12-4f49-99fc-97b44336692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 문장(질문)의 최대 길이를 설정합니다.\n",
    "# 입력된 문장이 이 길이를 초과하면 뒷부분을 잘라냅니다.\n",
    "# 예: 최대 길이가 64일 때, 70 단어의 질문은 뒤의 6단어가 잘립니다.\n",
    "MAX_SRC_LEN = 64\n",
    "\n",
    "# 출력 문장(답변)의 최대 길이를 설정합니다.\n",
    "# 출력된 문장이 이 길이를 초과하면 뒷부분을 잘라냅니다.\n",
    "# 예: 최대 길이가 64일 때, 70 단어의 답변은 뒤의 6단어가 잘립니다.\n",
    "MAX_TRG_LEN = 64\n",
    "\n",
    "# 입력 문장을 숫자로 바꿀 때 무작위로 일부 단어를 '<unk>'(알 수 없는 단어)로 바꾸는 확률입니다.\n",
    "# 예: \"안녕 오늘 날씨 어때?\"라는 문장을 숫자로 바꿀 때 0.05(5%) 확률로 일부 단어가 '<unk>'가 될 수 있습니다.\n",
    "SRC_TOKEN_DROPOUT_P = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6674c9c-c139-4c67-8ee6-06e82bd5211a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>귀가 가려워</td>\n",
       "      <td>누가 욕하고 있나봐요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2742</th>\n",
       "      <td>시트지로 리폼해야겠다</td>\n",
       "      <td>직접 하면 더 애착이 갈 거 같아요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>느낌이 좋아</td>\n",
       "      <td>사랑의 느낌이길 바라요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>대놓고 질린대</td>\n",
       "      <td>잠시 거리를 두고 생각해보세요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7955</th>\n",
       "      <td>저주스럽다 모든게 다</td>\n",
       "      <td>불행한 생각은 덜 하는게 당신에게 좋아요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>좋아했지만 고백은 못하겠어</td>\n",
       "      <td>애틋한 사랑이네요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>화장실</td>\n",
       "      <td>화장실 가세요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>6개월이 지나도 왜이런거죠</td>\n",
       "      <td>물리적 시간에 비례하지 않으니까요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>내가 제일 문제인 듯</td>\n",
       "      <td>당신은 하나밖에 없는 소중한 사람이에요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>요즘은 솔로인게 좋네</td>\n",
       "      <td>때론 혼자인게 편할때가 있죠</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10049 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Q                       A  label\n",
       "299            귀가 가려워             누가 욕하고 있나봐요      0\n",
       "2742      시트지로 리폼해야겠다     직접 하면 더 애착이 갈 거 같아요      0\n",
       "1090           느낌이 좋아            사랑의 느낌이길 바라요      0\n",
       "1194          대놓고 질린대        잠시 거리를 두고 생각해보세요      0\n",
       "7955      저주스럽다 모든게 다  불행한 생각은 덜 하는게 당신에게 좋아요      1\n",
       "...               ...                     ...    ...\n",
       "11284  좋아했지만 고백은 못하겠어               애틋한 사랑이네요      2\n",
       "5191              화장실                 화장실 가세요      0\n",
       "5390   6개월이 지나도 왜이런거죠      물리적 시간에 비례하지 않으니까요      1\n",
       "860       내가 제일 문제인 듯   당신은 하나밖에 없는 소중한 사람이에요      0\n",
       "7270      요즘은 솔로인게 좋네         때론 혼자인게 편할때가 있죠      1\n",
       "\n",
       "[10049 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아래는 하나의 데이터 예시를 표현하는 데이터 클래스입니다.\n",
    "# 하나의 데이터 예시는 입력(src)과 출력(trg) 숫자 리스트로 구성됩니다.\n",
    "@dataclass\n",
    "class Example:\n",
    "    # 입력 문장을 숫자로 표현한 리스트입니다.\n",
    "    # 예: 질문이 \"안녕 오늘 날씨 어때\"라면 숫자 리스트는 [1, 100, 45, 32, 2] 와 같이 됩니다.\n",
    "    # 여기서 1은 <sos>(문장 시작), 2는 <eos>(문장 끝)을 의미합니다.\n",
    "    src: List[int]\n",
    "\n",
    "    # 출력 문장을 숫자로 표현한 리스트입니다.\n",
    "    # 예: 답변이 \"오늘은 맑아\"라면 숫자 리스트는 [1, 45, 28, 2] 와 같이 됩니다.\n",
    "    # 마찬가지로 1은 문장 시작(<sos>), 2는 문장 끝(<eos>)입니다.\n",
    "    trg: List[int]\n",
    "\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    # QADataset 클래스는 질문(Q)과 대답(A) 데이터를\n",
    "    # 인공지능이 학습할 수 있는 숫자 형태로 바꾸어주는 역할을 합니다.\n",
    "    # Dataset은 학습용 PyTorch DataLoader에서 쉽게 사용할 수 있습니다.\n",
    "    # 즉, 이 클래스는 데이터를 ‘묶음(batch)’ 형태로 만들어주는 준비 단계입니다.\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, vocab: SpmVocab):\n",
    "        # QADataset 클래스의 생성자입니다.\n",
    "\n",
    "        # self.vocab은 클래스 내부에서 vocab을 사용하기 위해 저장하는 변수입니다.\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # self.samples: 질문-대답 쌍을 숫자 리스트 형태로 저장하는 리스트\n",
    "        self.samples: List[Example] = []\n",
    "\n",
    "        # df의 각 row를 가져와 숫자 리스트로 변환\n",
    "        for _, row in df.iterrows():\n",
    "\n",
    "            # 질문(Q)을 숫자 리스트로 변환\n",
    "            src_ids = vocab.encode(\n",
    "                row[\"Q\"],\n",
    "                add_sos=True,\n",
    "                add_eos=True,\n",
    "                max_len=MAX_SRC_LEN\n",
    "            )\n",
    "\n",
    "            # 대답(A)을 숫자 리스트로 변환\n",
    "            trg_ids = vocab.encode(\n",
    "                row[\"A\"],\n",
    "                add_sos=True,\n",
    "                add_eos=True,\n",
    "                max_len=MAX_TRG_LEN\n",
    "            )\n",
    "\n",
    "            # Example 객체로 묶어서 samples에 저장\n",
    "            self.samples.append(Example(src=src_ids, trg=trg_ids))\n",
    "\n",
    "    def __len__(self):\n",
    "        # 전체 데이터 개수를 반환\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # i번째 Example을 반환\n",
    "        return self.samples[i]\n",
    "\n",
    "\n",
    "def collate_fn(batch: List[Example], pad_idx: int = 0):\n",
    "    # collate_fn: 여러 개의 Example을 묶어서 텐서로 변환해 batch 형태로 만드는 함수\n",
    "\n",
    "    def pad_to_max(seqs: List[List[int]]):\n",
    "        # 가장 긴 길이에 맞춰 padding 수행\n",
    "        mx = max(len(s) for s in seqs)\n",
    "        return [s + [pad_idx] * (mx - len(s)) for s in seqs]\n",
    "\n",
    "    # 질문(src) padding\n",
    "    src = pad_to_max([b.src for b in batch])\n",
    "\n",
    "    # 대답(trg) padding\n",
    "    trg = pad_to_max([b.trg for b in batch])\n",
    "\n",
    "    # 텐서 변환\n",
    "    src = torch.tensor(src, dtype=torch.long)\n",
    "    trg = torch.tensor(trg, dtype=torch.long)\n",
    "\n",
    "    # trg_input: 마지막 토큰 제거\n",
    "    trg_input = trg[:, :-1]\n",
    "\n",
    "    # trg_output: 첫 토큰 제거\n",
    "    trg_output = trg[:, 1:]\n",
    "\n",
    "    # pad mask 생성\n",
    "    src_key_pad = (src == pad_idx)\n",
    "    trg_key_pad = (trg_input == pad_idx)\n",
    "\n",
    "    # 텐서 반환\n",
    "    return src, src_key_pad, trg_input, trg_output, trg_key_pad\n",
    "\n",
    "\n",
    "# train/valid split\n",
    "train_df, valid_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# train_df 표시\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c41db1a-b120-434a-bbf0-152b5c97d73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8169</th>\n",
       "      <td>죽을거 같네</td>\n",
       "      <td>나쁜 생각 하지 마세요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>내일 시험이야</td>\n",
       "      <td>컨디션 조절 하세요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8075</th>\n",
       "      <td>정말내 자신이 싫다</td>\n",
       "      <td>자신은 사랑해주세요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7625</th>\n",
       "      <td>이별후 네달째</td>\n",
       "      <td>바쁘게 살면서 잊어가요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2816</th>\n",
       "      <td>쌍커풀 해볼까</td>\n",
       "      <td>눈은 기본이죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11133</th>\n",
       "      <td>좋아하는 사람이 쓰레기면 어떻게 할꺼야</td>\n",
       "      <td>쓰레기 옆에 있기 싫어서 도망갈 거예요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4914</th>\n",
       "      <td>피곤해서 하루종일 누워있는 중</td>\n",
       "      <td>충전하는 시간 그 자체로 소중합니다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4417</th>\n",
       "      <td>차 살까</td>\n",
       "      <td>판단은 당신 몫이에요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>남편이 하나도 안 도와줘</td>\n",
       "      <td>돕는 게 아니라 같이 하는 거예요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7836</th>\n",
       "      <td>자기관리가 답인듯</td>\n",
       "      <td>못해봤던 것들을 해보세요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1774 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Q                      A  label\n",
       "8169                  죽을거 같네           나쁜 생각 하지 마세요      1\n",
       "900                  내일 시험이야             컨디션 조절 하세요      0\n",
       "8075              정말내 자신이 싫다             자신은 사랑해주세요      1\n",
       "7625                 이별후 네달째           바쁘게 살면서 잊어가요      1\n",
       "2816                 쌍커풀 해볼까                눈은 기본이죠      0\n",
       "...                      ...                    ...    ...\n",
       "11133  좋아하는 사람이 쓰레기면 어떻게 할꺼야  쓰레기 옆에 있기 싫어서 도망갈 거예요      2\n",
       "4914        피곤해서 하루종일 누워있는 중    충전하는 시간 그 자체로 소중합니다      0\n",
       "4417                    차 살까            판단은 당신 몫이에요      0\n",
       "777            남편이 하나도 안 도와줘     돕는 게 아니라 같이 하는 거예요      0\n",
       "7836               자기관리가 답인듯          못해봤던 것들을 해보세요      1\n",
       "\n",
       "[1774 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cedee58-e052-44a2-a834-130c1e3d75ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>귀가 가려워</td>\n",
       "      <td>누가 욕하고 있나봐요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>시트지로 리폼해야겠다</td>\n",
       "      <td>직접 하면 더 애착이 갈 거 같아요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>느낌이 좋아</td>\n",
       "      <td>사랑의 느낌이길 바라요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>대놓고 질린대</td>\n",
       "      <td>잠시 거리를 두고 생각해보세요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>저주스럽다 모든게 다</td>\n",
       "      <td>불행한 생각은 덜 하는게 당신에게 좋아요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10044</th>\n",
       "      <td>좋아했지만 고백은 못하겠어</td>\n",
       "      <td>애틋한 사랑이네요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10045</th>\n",
       "      <td>화장실</td>\n",
       "      <td>화장실 가세요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10046</th>\n",
       "      <td>6개월이 지나도 왜이런거죠</td>\n",
       "      <td>물리적 시간에 비례하지 않으니까요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10047</th>\n",
       "      <td>내가 제일 문제인 듯</td>\n",
       "      <td>당신은 하나밖에 없는 소중한 사람이에요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10048</th>\n",
       "      <td>요즘은 솔로인게 좋네</td>\n",
       "      <td>때론 혼자인게 편할때가 있죠</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10049 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Q                       A  label\n",
       "0              귀가 가려워             누가 욕하고 있나봐요      0\n",
       "1         시트지로 리폼해야겠다     직접 하면 더 애착이 갈 거 같아요      0\n",
       "2              느낌이 좋아            사랑의 느낌이길 바라요      0\n",
       "3             대놓고 질린대        잠시 거리를 두고 생각해보세요      0\n",
       "4         저주스럽다 모든게 다  불행한 생각은 덜 하는게 당신에게 좋아요      1\n",
       "...               ...                     ...    ...\n",
       "10044  좋아했지만 고백은 못하겠어               애틋한 사랑이네요      2\n",
       "10045             화장실                 화장실 가세요      0\n",
       "10046  6개월이 지나도 왜이런거죠      물리적 시간에 비례하지 않으니까요      1\n",
       "10047     내가 제일 문제인 듯   당신은 하나밖에 없는 소중한 사람이에요      0\n",
       "10048     요즘은 솔로인게 좋네         때론 혼자인게 편할때가 있죠      1\n",
       "\n",
       "[10049 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset_index 함수를 사용해서 데이터의 인덱스를 다시 0부터 시작하게 재정렬합니다.\n",
    "# drop=True는 기존의 인덱스를 제거하고, inplace=True는 실제 데이터프레임에 바로 적용한다는 뜻입니다.\n",
    "# 예를 들어 인덱스가 [1,4,7,9]였다면 재정렬 후엔 [0,1,2,3]으로 바뀌게 됩니다.\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_df   # 재정렬된 인덱스를 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "088b0fd6-5953-4f12-b1e3-d219a9fb53c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>죽을거 같네</td>\n",
       "      <td>나쁜 생각 하지 마세요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>내일 시험이야</td>\n",
       "      <td>컨디션 조절 하세요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>정말내 자신이 싫다</td>\n",
       "      <td>자신은 사랑해주세요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이별후 네달째</td>\n",
       "      <td>바쁘게 살면서 잊어가요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>쌍커풀 해볼까</td>\n",
       "      <td>눈은 기본이죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>좋아하는 사람이 쓰레기면 어떻게 할꺼야</td>\n",
       "      <td>쓰레기 옆에 있기 싫어서 도망갈 거예요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>피곤해서 하루종일 누워있는 중</td>\n",
       "      <td>충전하는 시간 그 자체로 소중합니다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>차 살까</td>\n",
       "      <td>판단은 당신 몫이에요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>남편이 하나도 안 도와줘</td>\n",
       "      <td>돕는 게 아니라 같이 하는 거예요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>자기관리가 답인듯</td>\n",
       "      <td>못해봤던 것들을 해보세요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1774 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Q                      A  label\n",
       "0                    죽을거 같네           나쁜 생각 하지 마세요      1\n",
       "1                   내일 시험이야             컨디션 조절 하세요      0\n",
       "2                정말내 자신이 싫다             자신은 사랑해주세요      1\n",
       "3                   이별후 네달째           바쁘게 살면서 잊어가요      1\n",
       "4                   쌍커풀 해볼까                눈은 기본이죠      0\n",
       "...                     ...                    ...    ...\n",
       "1769  좋아하는 사람이 쓰레기면 어떻게 할꺼야  쓰레기 옆에 있기 싫어서 도망갈 거예요      2\n",
       "1770       피곤해서 하루종일 누워있는 중    충전하는 시간 그 자체로 소중합니다      0\n",
       "1771                   차 살까            판단은 당신 몫이에요      0\n",
       "1772          남편이 하나도 안 도와줘     돕는 게 아니라 같이 하는 거예요      0\n",
       "1773              자기관리가 답인듯          못해봤던 것들을 해보세요      1\n",
       "\n",
       "[1774 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "valid_df   # 검증 데이터도 마찬가지로 재정렬된 인덱스를 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e242a224-60f3-4500-914d-9ab5d5e46503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.QADataset at 0x28264da7110>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이제 실제 인공지능 모델이 학습할 수 있는 형태의 데이터셋을 만듭니다.\n",
    "# train_ds는 학습용 데이터셋입니다. QADataset 클래스를 이용해 만들고, 숫자로 변환된 질문과 대답 데이터를 담고 있습니다.\n",
    "# 예: \"오늘 날씨 어때?\" → [1,23,45,2] 와 같은 숫자 형태로 바뀌어 저장됩니다.\n",
    "train_ds = QADataset(train_df, vocab)\n",
    "train_ds   # 학습용 데이터셋을 화면에서 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73f3f641-224f-4fe9-97d1-81e806427fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.QADataset at 0x2826cc666d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# valid_ds는 검증용 데이터셋으로 같은 방식으로 만듭니다. 검증용 데이터는 학습이 아니라 모델의 성능을 평가하는 용도로 쓰입니다.\n",
    "valid_ds = QADataset(valid_df, vocab)\n",
    "\n",
    "valid_ds   # 검증용 데이터셋을 화면에서 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95912e23-ea6c-4313-8c3e-8c41505dc0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab.pad_idx는 문장의 길이를 맞추기 위한 빈칸(<pad>)의 숫자값입니다. 여기서는 0입니다.\n",
    "# 예를 들어 [1,23,2]라는 문장을 길이 5로 맞추려면 뒤에 [0,0]을 붙여 [1,23,2,0,0]으로 만듭니다.\n",
    "vocab.pad_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74fa0c99-71b3-45ee-b5e2-d299760fcfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader는 데이터를 묶음(batch)으로 나누고, 학습 시 편하게 데이터를 사용할 수 있게 해주는 역할을 합니다.\n",
    "# train_loader는 학습용 데이터를 묶음 형태로 제공합니다.\n",
    "# batch_size=BATCH_SIZE는 묶음의 크기입니다(예: 64개의 데이터를 한번에 묶어서 제공).\n",
    "# shuffle=True는 데이터를 섞어서 매번 다른 순서로 제공합니다. 그래야 모델이 데이터를 골고루 학습할 수 있습니다.\n",
    "# collate_fn은 데이터를 묶어서 패딩을 추가하고 최종적으로 텐서 형태로 만듭니다.\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=lambda b: collate_fn(b, vocab.pad_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c68fbd47-4038-4f98-9e2f-32210de2f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_loader는 검증용 데이터를 묶음 형태로 제공합니다.\n",
    "# shuffle=False로 검증용 데이터는 섞지 않고 매번 같은 순서로 제공합니다(성능 평가를 일관되게 하려는 목적입니다).\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: collate_fn(b, vocab.pad_idx)\n",
    ")\n",
    "\n",
    "\n",
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model=256,\n",
    "                 num_encoder_layers=3, num_decoder_layers=3,\n",
    "                 dropout=0.2, pad_idx=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pad_idx = pad_idx\n",
    "        self.d_model = d_model\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "\n",
    "        # === 여기 수정 ===\n",
    "        # src / tgt 임베딩 이름을 src_embed / tgt_embed 로 통일\n",
    "        self.src_embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.tgt_embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=d_model,\n",
    "            hidden_size=d_model,\n",
    "            num_layers=num_encoder_layers,\n",
    "            dropout=dropout if num_encoder_layers > 1 else 0.0,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        self.dec_cells = nn.ModuleList(\n",
    "            [nn.LSTMCell(d_model, d_model) for _ in range(num_decoder_layers)]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.generator = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        # weight tying\n",
    "        self.generator.weight = self.tgt_embed.weight\n",
    "\n",
    "\n",
    "\n",
    "    def encode(self, src: torch.Tensor, src_key_padding_mask: torch.Tensor):\n",
    "        src = src.transpose(0, 1)\n",
    "        emb = self.dropout(self.src_embed(src))\n",
    "\n",
    "        lengths = (~src_key_padding_mask).sum(dim=1).clamp(min=1).cpu()\n",
    "        packed = pack_padded_sequence(emb, lengths, enforce_sorted=False)\n",
    "\n",
    "        _, (h_n, c_n) = self.encoder(packed)\n",
    "\n",
    "        h0 = h_n[-1]\n",
    "        c0 = c_n[-1]\n",
    "        return h0, c0\n",
    "\n",
    "\n",
    "    def decode(self, tgt: torch.Tensor, memory, tgt_key_padding_mask: torch.Tensor):\n",
    "        h0, c0 = memory\n",
    "        tgt = tgt.transpose(0, 1)   # (B, T) -> (T, B)\n",
    "        T, B = tgt.size(0), tgt.size(1)\n",
    "\n",
    "        h_states, c_states = [], []\n",
    "        for l in range(self.num_decoder_layers):\n",
    "            if l == 0:\n",
    "                h_states.append(h0)\n",
    "                c_states.append(c0)\n",
    "            else:\n",
    "                h_states.append(torch.zeros(B, self.d_model, device=tgt.device))\n",
    "                c_states.append(torch.zeros(B, self.d_model, device=tgt.device))\n",
    "\n",
    "        logits_steps = []\n",
    "\n",
    "        for t in range(T):\n",
    "            y_t = tgt[t]  # (B,)\n",
    "            # === 여기서도 tgt_embed 사용 ===\n",
    "            emb = self.dropout(self.tgt_embed(y_t))\n",
    "\n",
    "            step_mask = None\n",
    "            if tgt_key_padding_mask is not None:\n",
    "                step_mask = (~tgt_key_padding_mask[:, t]).unsqueeze(1)\n",
    "\n",
    "            x = emb\n",
    "\n",
    "            # 디코더 LSTMCell 스택 처리\n",
    "            for l, cell in enumerate(self.dec_cells):\n",
    "                h_prev, c_prev = h_states[l], c_states[l]\n",
    "                nh, nc = cell(x, (h_prev, c_prev))\n",
    "\n",
    "                if step_mask is not None:\n",
    "                    h_states[l] = torch.where(step_mask, nh, h_prev)\n",
    "                    c_states[l] = torch.where(step_mask, nc, c_prev)\n",
    "                else:\n",
    "                    h_states[l], c_states[l] = nh, nc\n",
    "\n",
    "                x = self.dropout(h_states[l])\n",
    "\n",
    "            h_top = x\n",
    "            logits_t = self.generator(h_top)\n",
    "            logits_steps.append(logits_t)\n",
    "\n",
    "        logits = torch.stack(logits_steps, dim=0)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def forward(self, src, src_key_padding_mask, tgt_input, tgt_key_padding_mask):\n",
    "        memory = self.encode(src, src_key_padding_mask)\n",
    "        logits = self.decode(tgt_input, memory, tgt_key_padding_mask)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07786a99-b2d4-4fe9-8adf-0f4f668bc02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMSeq2Seq 모델의 인스턴스를 생성(초기화)합니다.\n",
    "# 이 모델은 주어진 질문을 입력으로 받아서 적절한 답변을 출력하는 역할을 합니다.\n",
    "model = LSTMSeq2Seq(\n",
    "        vocab_size=vocab.size,               # 전체 사용 가능한 단어(토큰)의 개수입니다. 예: 8000개\n",
    "        d_model=256,                         # 각 단어가 숫자 벡터로 표현될 때의 벡터 길이(차원)입니다. 예: 256\n",
    "        num_encoder_layers=3,                # 인코더의 LSTM층을 몇 개 쌓을 것인지 설정합니다. 예: 3개 층\n",
    "        num_decoder_layers=3,                # 디코더의 LSTMCell 층 개수를 설정합니다. 예: 3개 층\n",
    "        dropout=0.2,                         # 드롭아웃 비율로, 일부 뉴런을 무작위로 끄는 비율입니다. 예: 20%\n",
    "        pad_idx=vocab.pad_idx                # 패딩(빈칸)의 숫자 인덱스입니다. 일반적으로 0으로 설정됩니다.\n",
    ").to(DEVICE)\n",
    "\n",
    "# ls_ce_loss는 손실 함수(loss function)로, 인공지능 모델이 얼마나 잘못 예측했는지 숫자로 표현합니다.\n",
    "# 손실 값이 낮을수록 인공지능 모델이 잘 예측하고 있는 것입니다.\n",
    "def ls_ce_loss(logits: torch.Tensor, targets: torch.Tensor, ignore_index: int, label_smoothing: float = 0.1):\n",
    "    # 매개변수 설명:\n",
    "    # logits: 인공지능이 예측한 각 단어의 점수(확률)입니다. 크기는 (T,B,V) → (단어 길이, 배치 크기, 전체 단어 수)입니다.\n",
    "    # targets: 실제 정답 문장의 단어들입니다. 크기는 (B,T) → (배치 크기, 단어 길이)입니다.\n",
    "    # ignore_index: 손실 계산 시 무시할 값(일반적으로 padding 값인 0)을 의미합니다.\n",
    "    # label_smoothing: 정답을 살짝 부드럽게(smoothed) 하여 모델이 정답만을 과신하지 않게 돕습니다(기본값 0.1).\n",
    "\n",
    "    logits = logits.transpose(0,1).contiguous()     # 크기를 (B,T,V) 형태로 변경합니다.\n",
    "    V = logits.size(-1)                              # 전체 단어의 수(V)를 가져옵니다.\n",
    "    logits = logits.view(-1, V)                      # logits을 2차원으로 평평하게 만들어 계산을 쉽게 합니다.\n",
    "                                                     # 예: (B×T,V), 전체 단어 확률을 평평하게 만듭니다.\n",
    "    targets = targets.reshape(-1)                    # 정답 단어들도 평평한 형태로 바꿉니다(B×T,).\n",
    "\n",
    "    # F.cross_entropy 함수가 label_smoothing을 지원하는지 확인합니다.\n",
    "    sig = inspect.signature(F.cross_entropy)\n",
    "    if \"label_smoothing\" in sig.parameters:\n",
    "        # cross_entropy 함수로 예측값(logits)과 정답(targets) 간의 손실을 계산합니다.\n",
    "        return F.cross_entropy(\n",
    "            logits,\n",
    "            targets,\n",
    "            ignore_index=ignore_index,\n",
    "            label_smoothing=label_smoothing\n",
    "        )\n",
    "\n",
    "    # 위의 방식이 안될 경우(fallback) 직접 계산합니다.\n",
    "    # log_softmax는 logits를 확률 값으로 바꿉니다.\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # 실제 정답 분포(true_dist)를 만듭니다. 전체 확률을 부드럽게(smoothing) 만듭니다.\n",
    "    with torch.no_grad():\n",
    "        # 전체 값에 아주 작은 확률(label_smoothing / (V - 1))을 줍니다.\n",
    "        true_dist = torch.zeros_like(log_probs)\n",
    "        true_dist.fill_(label_smoothing / (V - 1))\n",
    "    \n",
    "        # 정답 위치는 높은 확률(1 - label_smoothing)을 줍니다.\n",
    "        mask = (targets != ignore_index)   # 패딩 부분은 제외하고 계산합니다.\n",
    "        idx = targets[mask]\n",
    "        true_dist[mask, idx] = 1.0 - label_smoothing\n",
    "    \n",
    "    # 실제 분포(true_dist)와 예측 분포(log_probs) 차이를 계산하여 손실을 만듭니다.\n",
    "    loss = -(true_dist * log_probs).sum(dim=-1)\n",
    "    \n",
    "    # ignore_index(예: 패딩)가 아닌 값들만 손실을 계산하고 평균을 냅니다.\n",
    "    loss = loss[targets != ignore_index].mean()\n",
    "    \n",
    "    # 계산된 최종 손실 값을 반환합니다.\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "621d941a-8cfa-47e0-ac9e-7f445d3bda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    # NoamOpt 클래스는 인공지능 모델이 학습할 때 사용하는 옵티마이저(optimizer)를 조절하는 역할을 합니다.\n",
    "    # 이 클래스는 \"학습률 스케줄러(learning rate scheduler)\"라고도 부릅니다.\n",
    "    # 학습률(learning rate)이란 인공지능이 한 번 학습할 때마다 얼마나 빠르게 배우는지 결정하는 숫자입니다.\n",
    "    # 이 숫자가 너무 크면 학습이 불안정하고, 너무 작으면 학습이 너무 느려집니다.\n",
    "    # 이 클래스는 적절한 학습률을 자동으로 조절하여 안정적으로 학습할 수 있도록 도와줍니다.\n",
    "\n",
    "    # NoamOpt 학습률 계산 공식:\n",
    "    # 학습률 = factor × d_model^-0.5 × min(step^-0.5, step × warmup^-1.5)\n",
    "    # 이 공식은 처음에는 작은 학습률로 천천히 학습을 시작하고,\n",
    "    # 점점 빠르게 올라갔다가 특정 지점을 이후 다시 천천히 내려가게 만듭니다.\n",
    "\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000, factor=1.0, min_lr=1e-6):\n",
    "        # 클래스의 초기화 함수입니다. 이 함수는 클래스가 처음 만들어질 때 실행됩니다.\n",
    "        #\n",
    "        # 매개변수 설명:\n",
    "        # optimizer: 인공지능이 학습할 때 사용하는 기본 옵티마이저입니다(예: AdamW).\n",
    "        # d_model: 단어를 숫자 벡터로 표현한 벡터의 차원입니다(예: 256).\n",
    "        # warmup_steps: 학습률이 처음에 서서히 올라가는 단계(steps)의 수입니다. 기본값은 4000입니다.\n",
    "        # factor: 학습률 전체 크기를 조정하는 숫자입니다(기본값은 1.0이며, 학습 속도를 빠르게 또는 느리게 만듭니다).\n",
    "        # min_lr: 학습률의 최소값으로, 학습률이 이 값 이하로 내려가지 않습니다(기본값은 0.000001).\n",
    "\n",
    "        self.optimizer = optimizer         # 실제 사용하는 옵티마이저입니다.\n",
    "        self.d_model = d_model             # 벡터의 차원(d_model)을 저장합니다.\n",
    "        self.warmup = warmup_steps         # warmup 단계를 저장합니다.\n",
    "        self.factor = factor               # 학습률 크기를 조정하는 factor를 저장합니다.\n",
    "        self.min_lr = min_lr               # 최소 학습률을 저장합니다.\n",
    "\n",
    "        self._step = 0                     # 현재 학습한 횟수를 저장하는 변수입니다(초기값 0).\n",
    "        self._rate = 0.0                   # 현재 학습률을 저장합니다(초기값 0.0).\n",
    "\n",
    "    def step(self):\n",
    "        # step 함수는 한 번 학습할 때마다 자동으로 호출됩니다.\n",
    "        # 이 함수는 매번 학습률을 새로 계산해서 옵티마이저에 전달합니다.\n",
    "\n",
    "        self._step += 1                       # 학습을 진행할 때마다 step을 하나씩 증가시킵니다.\n",
    "        rate = self.rate()                    # 현재 학습률을 계산합니다.\n",
    "\n",
    "        # 옵티마이저가 사용하는 모든 파라미터 그룹에 새 학습률(rate)을 적용합니다.\n",
    "        # 학습률은 최소값(min_lr)보다 작아지지 않도록 유지합니다.\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = max(self.min_lr, rate)\n",
    "\n",
    "        self._rate = rate                     # 현재 사용된 학습률을 저장합니다.\n",
    "        self.optimizer.step()                 # 실제 옵티마이저의 step 함수를 호출하여 학습을 진행합니다.\n",
    "\n",
    "    def zero_grad(self):\n",
    "        # zero_grad 함수는 다음번 학습을 시작하기 전에 인공지능 모델의 기울기(gradient)를 0으로 초기화합니다.\n",
    "        # 기울기를 초기화하는 매 학습마다 새롭게 기울기를 계산하기 위해 필수입니다.\n",
    "\n",
    "        self.optimizer.zero_grad()            # 실제 옵티마이저의 기울기를 모두 0으로 초기화합니다.\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        # rate 함수는 주어진 학습 단계(step)에 따라 학습률을 계산하여 반환합니다.\n",
    "        # 이 함수는 학습률 스케줄러 공식을 따라 계산합니다.\n",
    "        # 매개변수 step: 학습률을 계산할 때 사용할 단계(steps)입니다.\n",
    "        #   → step 값이 따로 주어지지 않으면, 현재의 self._step 값을 사용합니다.\n",
    "\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "\n",
    "        # 학습률을 공식에 따라 계산합니다.\n",
    "        calculated_rate = (\n",
    "            self.factor\n",
    "            * (self.d_model ** -0.5)\n",
    "            * min(step ** -0.5, step * (self.warmup ** -1.5))\n",
    "        )\n",
    "\n",
    "        # 예를 들어 d_model이 256이고, warmup_steps가 4000이며, 현재 step이 2000일 때,\n",
    "        # 계산된 학습률은 factor × (256^(-0.5) × min(2000^(-0.5), 2000 × 4000^(-1.5)) 와 같이 계산됩니다.\n",
    "\n",
    "        return calculated_rate                # 계산된 학습률을 반환합니다.\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 🔥 n-gram 반복 방지 함수 (주석 온전하게 유지)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def ngram_ban(tokens: list[int], n: int) -> set[int]:\n",
    "    # ngram_ban 함수는 인공지능 모델이 문장을 생성할 때 같은 단어가 반복되는 현상을 방지하는 역할을 합니다.\n",
    "    # 이 현상을 “n-gram 반복 방지”라고 합니다.\n",
    "    # 여기서 n-gram은 ‘연속 n개의 단어’를 의미합니다.\n",
    "    # 예를 들어, 3-gram(\"나는 밥을 먹었다\")이면 [\"나는\", \"밥을\", \"먹었다\"]처럼 3개 단어가 묶인 의미입니다.\n",
    "\n",
    "    # 매개변수 설명:\n",
    "    # tokens: 지금까지 생성된 문장(단어들)을 숫자 형태로 표현한 리스트입니다.\n",
    "    #        예: [1, 5, 7, 10, 7, 12]\n",
    "    # n: n-gram의 크기. 몇 개의 연속된 단어가 반복되지 않도록 막을지를 결정합니다.\n",
    "    #    예: n=3 이라면, 같은 3개 단어 묶음이 연달아 나타나지 않도록 하는 기능입니다.\n",
    "\n",
    "    # n이 2보다 작거나 문장의 길이가 n-1보다 짧으면 반복을 검사할 필요가 없으므로 빈 set(금지어)를 반환합니다.\n",
    "    if n < 2 or len(tokens) < n - 1:\n",
    "        return set()\n",
    "\n",
    "    table = {}   # 반복되는 단어 묶음을 기록할 테이블(딕셔너리)입니다.\n",
    "\n",
    "    # 문장을 앞에서부터 한 칸씩 옮겨가면서 (n-1)개 단어로 이루어진 묶음(prefix)을 만듭니다.\n",
    "    # 같은 prefix에 바로 뒤에 오는 단어(next)를 기록합니다.\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        prefix = tuple(tokens[i : i + n - 1])  # 현재 위치에서 n-1개의 단어 묶음을 만듭니다.\n",
    "        nxt = tokens[i + n - 1]                # 이 묶음 뒤에 오는 바로 다음 단어를 가져옵니다.\n",
    "\n",
    "        # 만약 prefix가 table에 없다면 새로운 빈 집합을 넣습니다.\n",
    "        # 이미 있으면 set에 새로운 다음 단어(nxt)를 추가해줍니다.\n",
    "        table.setdefault(prefix, set()).add(nxt)\n",
    "\n",
    "    # 예: tokens=[1,5,7,10,5,7,12], n=3일 때,\n",
    "    #     prefix (1,5) 다음에 7이 나타났으면 table[(1,5)]={7}이 됩니다.\n",
    "    #     prefix (5,7) 뒤에 10이 오고 12가 나타났으면 table[(5,7)]={10,12}가 됩니다.\n",
    "\n",
    "    # 이제 문장의 마지막 (n-1)개 단어를 prefix로 사용해서\n",
    "    # 해당 prefix 뒤에 등장하면 안 되는(이미 반복되었던) 단어 목록을 찾습니다.\n",
    "    # 이렇게 반환된 단어들은 금지해야 하는 단어들로, 모델의 생성 제한 용도로 쓰입니다.\n",
    "    pref = tuple(tokens[len(tokens) - (n - 1) :])\n",
    "\n",
    "    # prefix 뒤에 이미 등장했던 단어가 있으면 단어 집합을 반환합니다.\n",
    "    # 없으면 빈 set 반환.\n",
    "    return table.get(pref, set())\n",
    "\n",
    "    # 예: tokens=[1,5,7,10,5,7,10], n=3일 때,\n",
    "    # 마지막 prefix=(5,7) 뒤에 10이 여러 번 등장했으므로 {10}을 반환합니다.\n",
    "    # 즉, 다음 단어로 10이 다시 나오면 안 되므로 그걸 방지하는 용도로 사용됩니다.\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reply(model: TransformerSeq2Seq, text: str, max_len: int = 64):\n",
    "        # reply 함수는 학습된 인공지능 모델이 사용자의 입력 문장에 적절한 답변을 만들어 주는 기능을 합니다.\n",
    "        # 이 함수는 실제 서비스에서 사용자가 챗봇에게 질문하면, 챗봇이 답변을 만들어주는 역할을 합니다.\n",
    "        #\n",
    "        # 매개변수 설명:\n",
    "        # model: 답변을 만드는 데 사용할 학습된 인공지능 모델입니다.\n",
    "        # text: 사용자 입력 문장(질문)입니다. 예: \"오늘 날씨 어때?\"\n",
    "        # max_len: 생성할 답변 문장의 최대 길이입니다. 기본값은 64단어입니다.\n",
    "        #\n",
    "        # @torch.no_grad()는 이 함수 내부에서는 기울기(gradient)를 계산하지 않고,\n",
    "        # 오직 문장을 만드는 출력용(inference) 작업만 하도록 설정하는 것입니다.\n",
    "        # 실제 서비스에서 문장을 만들 때만 사용하는 기능이며, 학습할 때는 사용하지 않습니다.\n",
    "    \n",
    "        # generate_beam 함수는 \"빔 서치(beam search)\"라는 방법을 이용해서 좋은 문장을 만듭니다.\n",
    "        # 빔 서치란, 단어를 하나씩 예측해가면서 여러 가지 후보 문장을 동시에 가장 좋은 문장을 찾는 방법입니다.\n",
    "        # beam_size=4는 한 번에 4가지 문장 후보를 기억하면서 탐색한다는 뜻입니다.\n",
    "        # max_len=64는 생성할 문장의 최대 길이이며, min_len=6는 최소 길이를 의미합니다.\n",
    "        # alpha=0.6은 문장 길이에 따른 점수를 조절하는 비율이며, 길이를 0.6으로 설정하여 길이 적절한 문장을 생성하게 합니다.\n",
    "        # no_repeat_ngram_size=3은 3개의 연속된 단어가 똑같이 다시 반복되는 것을 방지하는 기능이고,\n",
    "        # repetition_penalty=1.2는 같은 단어가 자꾸 반복되는 것을 줄이기 위한 기능입니다.\n",
    "    \n",
    "        ids = generate_beam(\n",
    "            model,\n",
    "            text,\n",
    "            beam_size=4,\n",
    "            max_len=max_len,\n",
    "            min_len=6,\n",
    "            alpha=0.6,\n",
    "            no_repeat_ngram_size=3,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "    \n",
    "        # 생성된 문장(ids)에서 처음에 있는 시작 토큰(<sos>)을 제거하고,\n",
    "        # 끝에 있는 문장 끝(<eos>) 이후의 단어들도 제거합니다.\n",
    "        # 모델이 \"답변의 시작\"을 표시하기 위해 <sos>를 넣기 때문입니다.\n",
    "        # 예를 들어, ids가 [1, 34, 67, 23, 2, 89,10], 10이 <sos>, 2가 <eos>라면,\n",
    "        # 1 제거 후: [34, 67, 23, 2, 89,10]\n",
    "        # 2(eos)가 나오면 그 이후는 의미 없으므로 잘라냅니다.\n",
    "    \n",
    "        # 첫 단어가 <sos> 토큰이면 제거\n",
    "        if ids and ids[0] == vocab.sos_idx:\n",
    "            ids = ids[1:]\n",
    "    \n",
    "        # 문장 끝 토큰(<eos>)이 있으면 그 뒤를 자른다\n",
    "        if vocab.eos_idx in ids:\n",
    "            ids = ids[: ids.index(vocab.eos_idx)]\n",
    "    \n",
    "        # 숫자로 이루어진 문장(ids)을 실제 사람이 읽을 수 있는 텍스트 문장으로 변환합니다.\n",
    "        # 예: [34, 67, 23] → \"날씨 맑아요\"\n",
    "        return vocab.decode(ids)      # 최종 생성된 문장을 반환합니다.\n",
    "\n",
    "    def run_epoch(model, loader, optimizer: NoamOpt | None,\n",
    "                  label_smoothing=0.1, src_token_dropout=0.05, grad_clip=1.0):\n",
    "        # run_epoch 함수는 인공지능 모델에 데이터를 이용해 한 번(epoch) 학습하거나 성능을 평가할 때 사용하는 함수입니다.\n",
    "        # 이 함수는 학습용 데이터(트레이닝 데이터)나 검증용 데이터(밸리데이션 데이터)를 한 번 전체 순회하는 역할을 합니다.\n",
    "        #\n",
    "        # 매개변수 설명:\n",
    "        # model: 인공지능 모델입니다(LSTMSeq2Seq 모델).\n",
    "        # loader: DataLoader로 묶은 데이터를 묶음(batch)들 순서대로 제공합니다.\n",
    "        # optimizer: 모델의 학습을 관리는 옵티마이저 객체입니다.\n",
    "        #            optimizer가 None이면 모델을 평가(검증)만 하고 학습은 하지 않습니다.\n",
    "        # label_smoothing: 손실(loss)을 계산할 때 정답의 확률 분포를 부드럽게 만들어\n",
    "        #                  모델이 정답을 과신하지 않게 하는 정규화 역할입니다.\n",
    "        #                  기본값은 0.1입니다.\n",
    "        # src_token_dropout: 입력 데이터(src)의 일부 단어를 무작위로 보이지 않는 단어(<unk>)로 바꾸어\n",
    "        #                    모델이 더 강인하게 학습하게 합니다.\n",
    "        #                    기본값은 0.05(5%)입니다.\n",
    "        # grad_clip: 기울기(gradient)를 일정 크기로 제한하여 학습이 폭발적으로 증가하거나\n",
    "        #            불안정해지는 것을 막아줍니다.\n",
    "        #            기본값은 1.0입니다.\n",
    "    \n",
    "        is_train = optimizer is not None        # optimizer가 있으면 훈련(train)모드, 없으면 평가(eval) 모드입니다.\n",
    "        model.train(is_train)                   # 모델을 훈련 모드 또는 평가 모드로 설정합니다.\n",
    "    \n",
    "        total, n = 0.0, 0                       # 손실(loss)의 합(total)과 데이터 개수(n)를 저장하는 변수입니다.\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def reply(model: TransformerSeq2Seq, text: str, max_len: int = 64):\n",
    "            # reply 함수는 학습된 인공지능 모델이 사용자의 입력 문장에 적절한 답변을 만들어 주는 기능을 합니다.\n",
    "            # 이 함수는 실제 서비스에서 사용자가 챗봇에 질문하면, 챗봇이 답변을 만들어주는 역할을 합니다.\n",
    "            #\n",
    "            # 매개변수 설명:\n",
    "            # model: 답변을 만드는 데 사용할 학습된 인공지능 모델입니다.\n",
    "            # text: 사용자가 입력한 문장(질문)입니다. 예: \"오늘 날씨 어때?\"\n",
    "            # max_len: 생성할 답변 문장의 최대 길이입니다. 기본값은 64단어입니다.\n",
    "        \n",
    "            # @torch.no_grad()는 이 함수 내부에서는 기울기(gradient)를 계산하지 않고,\n",
    "            # 오직 문장을 만드는 추론(inference) 작업만 하도록 설정하는 것입니다.\n",
    "            # 실제 서비스에서는 문장을 만들 때만 사용하는 기능이며, 학습할 때는 사용하지 않습니다.\n",
    "        \n",
    "            # generate_beam 함수는 '빔 서치(beam search)'라는 방법을 이용해서 좋은 문장을 만듭니다.\n",
    "            # 빔 서치란, 단어를 하나씩 예측할 때 여러 개의 후보 문장을 동시에 평가해서\n",
    "            # 가장 좋은 문장을 찾는 방법입니다.\n",
    "            # beam_size=4는 한번에 최대 4가지 문장 후보를 기억하면서 탐색한다는 뜻입니다.\n",
    "            # max_len=64는 생성할 문장의 최대 길이이고, min_len=6은 최소 길이를 의미합니다.\n",
    "            # alpha는 문장 길이에 대한 패널티이며, 기본값 0.6으로 설정되어 길이 적절한 문장을 생성하게 합니다.\n",
    "            # no_repeat_ngram_size=3은 3개 단어가 연달아 반복되지 않도록 막아주는 기능입니다.\n",
    "            # repetition_penalty=1.2는 같은 단어가 자꾸 반복되는 것을 줄이기 위한 기능입니다.\n",
    "        \n",
    "            ids = generate_beam(\n",
    "                model, text,\n",
    "                beam_size=4,\n",
    "                max_len=max_len,\n",
    "                min_len=6,\n",
    "                alpha=0.6,\n",
    "                no_repeat_ngram_size=3,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "        \n",
    "            # 생성된 문장(ids)에서 처음에 있는 시작 토큰(<sos>)을 제거하고,\n",
    "            # 문장 끝 토큰(<eos>)이 있으면 단어들을 잘라냅니다.\n",
    "            # 이렇게 처리하는 이유는 <sos>, <eos>는 사람이 쓰지 않기 때문입니다.\n",
    "            # 예를 들어, ids가 [1, 34, 67, 23, 2, 89, 10], 1이 <sos>, 2가 <eos>라면,\n",
    "            # 정답 ids는 [34, 67, 23]로 줄어듭니다.\n",
    "        \n",
    "            if ids and ids[0] == vocab.sos_idx:    # 첫 단어가 시작 토큰(<sos>)인지 확인합니다.\n",
    "                ids = ids[1:]                      # 시작 토큰을 제거합니다.\n",
    "        \n",
    "            if vocab.eos_idx in ids:               # 문장 끝 토큰(<eos>)이 있다면 그 위치를 찾습니다.\n",
    "                ids = ids[:ids.index(vocab.eos_idx)]  # 문장 끝 토큰 뒤의 단어들을 제거합니다.\n",
    "        \n",
    "            # 숫자로 이루어진 문장(ids)을 실제 사람이 읽을 수 있는 텍스트 문장으로 변환합니다.\n",
    "            return vocab.decode(ids)               # 최종 생성된 문장을 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39fae1f6-f0c3-49fc-83d9-59f9ec58b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, optimizer: NoamOpt | None,\n",
    "              label_smoothing=0.1, src_token_dropout=0.05, grad_clip=1.0):\n",
    "    # run_epoch 함수는 인공지능 모델이 데이터를 이용해 한 번(1 epoch) 학습하거나\n",
    "    # 성능을 평가할 때 사용하는 함수입니다.\n",
    "    # 이 함수는 학습용 데이터(트레이닝 데이터)나 검증용 데이터(밸리데이션 데이터)를\n",
    "    # 한 번 전체 순회하는 역할을 합니다.\n",
    "\n",
    "    # 매개변수 설명:\n",
    "    # model: 인공지능 모델입니다(LSTMSeq2Seq 모델).\n",
    "    # loader: DataLoader로 묶은 데이터 묶음(batch)을 순서대로 제공합니다.\n",
    "    # optimizer: 모델의 학습을 관리하는 도구입니다.\n",
    "    #            NoamOpt일 수도 있고, None일 수도 있습니다.\n",
    "    #            optimizer가 None이면 모델을 평가(검증)만 하고 학습은 하지 않습니다.\n",
    "    # label_smoothing: 손실(loss)을 계산할 때 정답의 확률 분포를 부드럽게 만들어\n",
    "    #                  모델이 정답을 과신하지 않게 하는 정도입니다.\n",
    "    #                  기본값은 0.1입니다.\n",
    "    # src_token_dropout: 입력 데이터(src)의 일부 단어를 무작위로 알 수 없는 단어(<unk>)로 바꿔\n",
    "    #                    모델이 더 강인하게 학습하게 합니다.\n",
    "    #                    기본값은 0.05(5%)입니다.\n",
    "    # grad_clip: 기울기(gradient)를 일정 크기로 제한하여\n",
    "    #            학습이 폭발적으로 증가하거나 불안정해지는 것을 막아줍니다.\n",
    "    #            기본값은 1.0입니다.\n",
    "\n",
    "    is_train = optimizer is not None       # optimizer가 있으면 훈련(train)모드, 없으면 평가(eval)모드입니다.\n",
    "    model.train(is_train)                  # 모델을 훈련 모드 또는 평가 모드로 설정합니다.\n",
    "\n",
    "    total, n = 0.0, 0                      # 손실(loss)의 합(total)과 데이터 개수(n)를 저장하는 변수입니다.\n",
    "   \n",
    "    # loader에서 한 번에 한 묶음(batch)의 데이터를 가져와서 반복합니다.\n",
    "    for (src, src_key_pad, trg_input, trg_output, trg_key_pad) in loader:\n",
    "\n",
    "        # 각 데이터를 GPU나 CPU 장치로 이동시킵니다.\n",
    "        src, src_key_pad = src.to(DEVICE), src_key_pad.to(DEVICE)\n",
    "        trg_input, trg_output, trg_key_pad = (\n",
    "            trg_input.to(DEVICE), trg_output.to(DEVICE), trg_key_pad.to(DEVICE)\n",
    "        )\n",
    "\n",
    "        # 훈련 시에만 입력(src) 문장의 일부 단어를 랜덤하게 알 수 없는 단어(<unk>)로 바꿉니다.\n",
    "        if is_train and src_token_dropout > 0:\n",
    "            # 랜덤한 값이 dropout 비율보다 작으면 해당 위치의 단어를 바꿉니다.\n",
    "            drop = (torch.rand_like(src, dtype=torch.float) < src_token_dropout)\n",
    "\n",
    "            # 패딩과 같은 특수 단어(<pad>, <sos>, <eos>)는 바꾸지 않고 유지합니다.\n",
    "            special = (\n",
    "                (src == vocab.pad_idx)\n",
    "                | (src == vocab.sos_idx)\n",
    "                | (src == vocab.eos_idx)\n",
    "            )\n",
    "\n",
    "            # 랜덤으로 선택된 일반 단어 위치를 <unk>(알 수 없는 단어)로 바꿉니다.\n",
    "            src = torch.where(drop & (~special),\n",
    "                              torch.full_like(src, vocab.unk_idx),\n",
    "                              src)\n",
    "            # 예를 들어 src가 [1, 23, 56, 2]이고, 56이 dropout 대상이면\n",
    "            # [1, 23, 3(<unk>), 2]로 바뀝니다.\n",
    "\n",
    "        # 모델에 데이터를 넣고 예측한 결과(logits)를 얻습니다.\n",
    "        # logits: 각 단어별 예측 확률로, 크기는 (T,B,V)입니다.\n",
    "        logits = model(src, src_key_pad, trg_input, trg_key_pad)\n",
    "\n",
    "        # 실제 정답(trg_output)과 모델의 예측(logits)을 비교하여 손실(loss)을 계산합니다.\n",
    "        loss = ls_ce_loss(logits, trg_output,\n",
    "                          ignore_index=vocab.pad_idx,\n",
    "                          label_smoothing=label_smoothing)\n",
    "        \n",
    "        # 학습 모드일 때는 실제로 학습을 진행합니다.\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()                       # 이전 학습 때 남은 기울기(gradient)를 0으로 초기화합니다.\n",
    "            loss.backward()                             # 손실을 줄이는 방향으로 기울기를 계산합니다.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)  # 기울기가 너무 커지지 않도록 제한합니다.\n",
    "            optimizer.step()                            # 계산된 기울기를 이용해 실제 모델 파라미터(가중치)를 업데이트(학습)합니다.\n",
    "\n",
    "        bs = src.size(0)                                # 현재 묶음(batch)의 크기입니다.\n",
    "        total += float(loss.item()) * bs                # 현재 묶음의 손실을 총 손실에 추가합니다.\n",
    "        n += bs                                         # 현재 묶음의 개수를 전체 개수에 추가합니다.\n",
    "\n",
    "    # 최종적으로 평균 손실을 계산해서 반환합니다.\n",
    "    # 손실값이 낮을수록 모델이 데이터를 잘 학습한 것입니다.\n",
    "    return total / max(1, n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de0cff65-9180-4104-b6f2-858e3264ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_beam(\n",
    "    model: LSTMSeq2Seq,\n",
    "    text: str,\n",
    "    beam_size=4,\n",
    "    max_len=64,\n",
    "    min_len=6,\n",
    "    alpha=0.6,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.2\n",
    ") -> List[int]:\n",
    "    # generate_beam 함수는 사용자가 입력한 문장에서 인공지능 모델이 가장 적절한 답변 문장을 생성하는 함수입니다.\n",
    "    # 이 때 \"빔 서치(beam search)\"라는 방법을 사용하여 여러 후보 문장을 비교해서 가장 좋은 문장을 선택하는 방식입니다.\n",
    "\n",
    "    # 매개변수 설명:\n",
    "    # model: 답변 생성에 사용할 인공지능 모델입니다(LSTMSeq2Seq).\n",
    "    # text: 사용자가 입력한 문장입니다. 예: \"오늘 날씨 어때?\"\n",
    "    # beam_size: 매번 가장 좋은 문장을 선택할 때, 몇 가지 후보 문장을 유지할지를 정하는 값입니다(기본값: 4).\n",
    "    # max_len: 생성할 수 있는 답변 문장의 최대 길이입니다(기본값: 64단어).\n",
    "    # min_len: 답변 문장의 최소 길이입니다. 최소 길이가 짧아지면 문장 끝(<eos>) 토큰을 생성하지 않도록 합니다(기본값: 6).\n",
    "    # alpha: 문장 길이에 따라 점수를 보정하는 길이 보너스(length penalty)를 결정하는 값입니다(기본값: 0.6).\n",
    "    # no_repeat_ngram_size: 같은 단어가 연속으로 반복되는 것을 방지하기 위한 단어 묶음 크기입니다(기본값: 3).\n",
    "    # repetition_penalty: 같은 단어가 반복될 때 점수를 깎는 비율입니다(기본값: 1.2).\n",
    "\n",
    "    # 입력한 문장(text)을 인공지능 모델이 이해할 수 있는 숫자 형태로 변환합니다.\n",
    "    # add_sos=True는 문장 시작 토큰(<sos>)을 추가, add_eos=True는 문장 끝 토큰(<eos>)을 추가하는 옵션입니다.\n",
    "    src_ids = vocab.encode(text, add_sos=True, add_eos=True, max_len=MAX_SRC_LEN)\n",
    "\n",
    "    # 숫자 리스트 형태의 문장을 텐서(Tensor) 형태로 변환하고 GPU(또는 CPU) 장치에 올립니다.\n",
    "    src = torch.tensor([src_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # 패딩(<pad>) 위치를 표시합니다. 여기서는 입력 문장(src) 중에서 pad_idx(일반적으로 0) 위치를 찾아 표시합니다.\n",
    "    src_key_pad = (src == vocab.pad_idx)\n",
    "\n",
    "    # 입력 문장을 인코더에 넣어서 문장의 의미를 담은 벡터(memory)를 얻습니다.\n",
    "    memory = model.encode(src, src_key_pad)\n",
    "\n",
    "    # beams 리스트는 문장을 생성하면서 유지할 후보 문장들입니다.\n",
    "    # 각 후보는 (지금까지 생성된 단어 리스트, 문장의 점수(logprob), 문장이 끝났는지 여부)로 이루어집니다.\n",
    "    beams = [([vocab.sos_idx], 0.0, False)]   # 처음에는 <sos> 토큰 하나 있는 문장으로 시작합니다.\n",
    "\n",
    "    # 단어를 하나씩 생성하면서 최대 길이(max_len)까지 반복합니다.\n",
    "    for step in range(max_len):\n",
    "\n",
    "        new_beams = []  \n",
    "        # 이번 단계에서 새롭게 생성될 후보 문장들입니다.\n",
    "\n",
    "        curr_tgts = [b[0] for b in beams]   \n",
    "        # 현재 후보 문장들을 가져옵니다.\n",
    "\n",
    "        lens = [len(t) for t in curr_tgts]  \n",
    "        # 각 후보 문장의 길이를 저장합니다.\n",
    "\n",
    "        T = max(lens)  \n",
    "        # 가장 긴 문장 길이를 찾습니다.\n",
    "\n",
    "        # 모든 후보 문장을 같은 길이로 맞추기 위해 짧은 문장 뒤에 패딩(<pad>)을 추가합니다.\n",
    "        tgt = []\n",
    "        tgt_keypad = []\n",
    "\n",
    "        for t in curr_tgts:\n",
    "            padded = t + [vocab.pad_idx] * (T - len(t))\n",
    "            tgt.append(padded)\n",
    "            tgt_keypad.append(([False] * len(t)) + ([True] * (T - len(t))))\n",
    "\n",
    "        # 후보 문장들을 텐서로 변환합니다.\n",
    "        tgt = torch.tensor(tgt, dtype=torch.long, device=DEVICE)\n",
    "        tgt_keypad = torch.tensor(tgt_keypad, dtype=torch.bool, device=DEVICE)\n",
    "\n",
    "        B = len(beams)  \n",
    "        # 현재 후보 문장의 개수입니다.\n",
    "\n",
    "        # 인코더의 출력을 후보 문장 수만큼 복사하여 사용합니다.\n",
    "        h0, c0 = memory\n",
    "        h0_rep = h0.expand(B, h0.size(1)).contiguous()\n",
    "        c0_rep = c0.expand(B, c0.size(1)).contiguous()\n",
    "        mem_rep = (h0_rep, c0_rep)\n",
    "\n",
    "        # 디코더를 사용하여 다음 단어의 점수(logits)를 얻습니다.\n",
    "        logits = model.decode(tgt, mem_rep, tgt_keypad)\n",
    "\n",
    "        last_logits = logits[-1]  \n",
    "        # 마지막으로 생성된 단어의 점수를 가져옵니다.\n",
    "\n",
    "        # 각 후보 문장에서 다음 단어를 선택합니다.\n",
    "        for i, (tokens, logp, ended) in enumerate(beams):\n",
    "\n",
    "            # ended=True 이면 이미 문장이 종료된 상태 → 그대로 유지합니다.\n",
    "            if ended:\n",
    "                new_beams.append((tokens, logp, True))\n",
    "                continue\n",
    "\n",
    "            scores = last_logits[i]    # 현재 후보 문장의 다음 단어 점수(logit)\n",
    "\n",
    "            # 패딩 토큰 <pad> 는 절대 선택되지 않도록 매우 낮은 점수로 설정합니다.\n",
    "            scores[vocab.pad_idx] = -1e9\n",
    "\n",
    "            # 시작 토큰 <sos> 도 출력에 나오면 안 되므로 선택 불가로 설정합니다.\n",
    "            scores[vocab.sos_idx] = -1e9\n",
    "\n",
    "            # 최소 길이에 도달하기 전에는 <eos> 토큰을 선택하지 못하도록 패널티 적용합니다.\n",
    "            if len(tokens) <= min_len:\n",
    "                scores[vocab.eos_idx] = -1e9\n",
    "\n",
    "            # 반복되는 단어를 방지하기 위해 repetition_penalty 를 적용해 점수를 보정합니다.\n",
    "            if repetition_penalty and repetition_penalty != 1.0:\n",
    "                seen = set(tokens)\n",
    "                for tid in list(seen):\n",
    "                    if scores[tid] > 0:\n",
    "                        scores[tid] = scores[tid] / repetition_penalty\n",
    "                    else:\n",
    "                        scores[tid] = scores[tid] * repetition_penalty\n",
    "\n",
    "            # 같은 n-gram 반복을 막기 위해 ngram_ban 함수로 금지어를 얻고 점수를 패널티 처리합니다.\n",
    "            if no_repeat_ngram_size and no_repeat_ngram_size >= 2:\n",
    "                bans = ngram_ban(tokens, no_repeat_ngram_size)\n",
    "                for tid in bans:\n",
    "                    scores[tid] = -1e9\n",
    "\n",
    "\n",
    "            # 다음 단어 확률 계산 → 가장 높은 단어 top-k 개 선택(beam_size 개)\n",
    "            log_probs = F.log_softmax(scores, dim=-1)\n",
    "            topk = torch.topk(log_probs, beam_size)\n",
    "\n",
    "            # top-k 후보들을 새로운 beam 후보로 저장합니다.\n",
    "            for k in range(beam_size):\n",
    "                tid = int(topk.indices[k])     # 다음 단어 id\n",
    "                logp_next = float(topk.values[k])\n",
    "                new_tokens = tokens + [tid]\n",
    "\n",
    "                # EOS 가 나오면 ended 상태로 전환합니다.\n",
    "                ended_flag = (tid == vocab.eos_idx)\n",
    "\n",
    "                new_beams.append((new_tokens, logp + logp_next, ended_flag))\n",
    "\n",
    "\n",
    "        # 문장 길이가 길어질수록 패널티(length penalty)를 적용해 최종 후보 점수를 조정합니다.\n",
    "        def length_penalty(L, alpha=0.6):\n",
    "            return ((5 + L) ** alpha) / ((5 + 1) ** alpha)\n",
    "\n",
    "        scored = []\n",
    "        for (ts, lp, ed) in new_beams:\n",
    "            score = lp / length_penalty(len(ts), alpha)\n",
    "            scored.append((score, ts, lp, ed))\n",
    "\n",
    "        # 점수 순으로 정렬하여 상위 beam_size 개만 다음 단계로 유지합니다.\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        beams = [(ts, lp, ed) for (_, ts, lp, ed) in scored[:beam_size]]\n",
    "\n",
    "        # 모든 beam 이 종료되었으면 탐색 중단합니다.\n",
    "        if all(ed for (_, _, ed) in beams):\n",
    "            break\n",
    "\n",
    "    # 최종적으로 가장 좋은 점수를 가진 문장을 선택해 반환합니다.\n",
    "    best = max(beams, key=lambda b: b[1] / length_penalty(len(b[0]), alpha))\n",
    "    return best[0]   # 최종 선택된 문장의 단어 id 리스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7860f1d-7197-4efb-a687-79f2c3afa073",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def reply(model: LSTMSeq2Seq, text: str, max_len: int = 64):\n",
    "    \"\"\"\n",
    "    학습된 LSTM Seq2Seq 모델로 답변 한 문장을 생성하는 함수.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # beam search로 토큰 id 시퀀스 생성\n",
    "    ids = generate_beam(\n",
    "        model, text,\n",
    "        beam_size=4,\n",
    "        max_len=max_len,\n",
    "        min_len=6,\n",
    "        alpha=0.6,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "    # 맨 앞 <sos> 제거\n",
    "    if ids and ids[0] == vocab.sos_idx:\n",
    "        ids = ids[1:]\n",
    "\n",
    "    # 중간에 <eos> 있으면 거기까지만 사용\n",
    "    if vocab.eos_idx in ids:\n",
    "        eos_pos = ids.index(vocab.eos_idx)\n",
    "        ids = ids[:eos_pos]\n",
    "\n",
    "    # id 시퀀스를 텍스트로 디코딩\n",
    "    return vocab.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4e5b656-3f0e-4cba-b43f-578326d3eff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 10049 / Valid size: 1774 / Vocab size (total): 8004 (SPM pieces: 8000)\n"
     ]
    }
   ],
   "source": [
    "# 아래는 인공지능 모델을 학습할 때 사용하는 중요한 설정값(하이퍼파라미터)들을 정의합니다.\n",
    "# 이 값들은 모델 학습이 어떻게 진행될지 세부적으로 결정해주는 역할을 합니다.\n",
    "\n",
    "epochs = 200\n",
    "# epochs(에폭)는 전체 데이터를 몇 번 반복해서 학습할지 결정하는 값입니다.\n",
    "# 예를 들어, epochs=200은 전체 데이터를 200번 학습하는 것을 의미합니다.\n",
    "\n",
    "base_lr = 1.0\n",
    "# base_lr(기본 학습률)은 모델이 학습할 때 사용하는 기본적인 학습률을 나타냅니다.\n",
    "# 이 값이 너무 크면 학습이 불안정하고, 너무 작으면 학습이 느리게 진행됩니다.\n",
    "# 기본값은 1.0을 사용하며, 이후 NoamOpt라는 학습률 조정을 사용해서 자동으로 조정됩니다.\n",
    "\n",
    "warmup_steps = 4000\n",
    "# warmup_steps는 학습률을 천천히 높이는 단계의 수입니다.\n",
    "# 초반에 낮은 학습률에서 시작해 이 단계까지 점점 학습률이 올라갑니다.\n",
    "# 예: warmup_steps=4000은 처음 4000번의 학습까지는 점점 더 큰 학습률을 사용하며,\n",
    "# 이후로는 다시 줄어듭니다.\n",
    "\n",
    "label_smoothing = 0.1\n",
    "# label_smoothing은 정답 레이블(정답 단어 확률)을 부드럽게 만들어서 모델이 특정 단어만 확신하지 않도록 합니다.\n",
    "# 예를 들어, label_smoothing=0.1이면 정답 확률을 90%로 낮추고, 나머지 확률을 다른 단어들에 나누어줍니다.\n",
    "\n",
    "src_token_dropout = 0.05\n",
    "# src_token_dropout은 입력 문장(src)의 일부 단어를 랜덤하게 <unk>(모르는 단어)로 바꾸는 비율입니다.\n",
    "# 예: 5% 확률로 [\"오늘\", \"날씨\", \"어때\"] → [\"오늘\", \"<unk>\", \"어때\"] 처럼 바뀌게 됩니다.\n",
    "# 이렇게 하면 모델이 더 다양한 상황에 잘 대응하게 됩니다.\n",
    "\n",
    "weight_decay = 1e-4\n",
    "# weight_decay는 모델의 가중치 값들이 너무 커지는 것을 막아 과적합(overfitting)을 방지하는 데 사용됩니다.\n",
    "# 일반적으로 아주 작은 값을 사용합니다(여기서는 0.0001).\n",
    "\n",
    "early_stop_patience = 10\n",
    "# early_stop_patience는 검증 데이터로 측정한 성능이 계속 개선되지 않을 때, 학습을 일찍 멈추는 기준입니다.\n",
    "# 예: patience가 10이면, 10번의 학습(epoch) 동안 성능이 전혀 좋아지지 않으면 학습을 종료합니다.\n",
    "\n",
    "save_path = \"seq2seq_lstm.pt\"\n",
    "# save_path는 학습된 모델을 저장할 파일의 경로(이름)입니다.\n",
    "# 학습이 끝나거나 중간에 좋은 성능이 나왔을 때 이 경로로 모델이 저장됩니다.\n",
    "\n",
    "# 현재 학습과 검증 데이터의 개수, 단어 사전(vocab)의 크기를 화면에 출력해서 확인합니다.\n",
    "print(f\"Train size: {len(train_ds)} / Valid size: {len(valid_ds)} / Vocab size (total): {vocab.size} (SPM pieces: {vocab.spm_size})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "931b4788-5327-4c4c-bce3-ac4609cb409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "[ep:0001/200] lr=3.90344e-05  train_loss=8.8200  valid_loss=8.1650\n",
      "====================================================================================================\n",
      "[ep:0002/200] lr=7.80687e-05  train_loss=8.0021  valid_loss=7.7461\n",
      "====================================================================================================\n",
      "[ep:0003/200] lr=0.000117103  train_loss=7.7581  valid_loss=7.5059\n",
      "====================================================================================================\n",
      "[ep:0004/200] lr=0.000156137  train_loss=7.5039  valid_loss=7.2149\n",
      "====================================================================================================\n",
      "[ep:0005/200] lr=0.000195172  train_loss=7.2266  valid_loss=6.9298\n",
      "====================================================================================================\n",
      "[ep:0006/200] lr=0.000234206  train_loss=6.9541  valid_loss=6.6844\n",
      "====================================================================================================\n",
      "[ep:0007/200] lr=0.000273241  train_loss=6.7021  valid_loss=6.4606\n",
      "====================================================================================================\n",
      "[ep:0008/200] lr=0.000312275  train_loss=6.4757  valid_loss=6.2747\n",
      "====================================================================================================\n",
      "[ep:0009/200] lr=0.000351309  train_loss=6.2846  valid_loss=6.1138\n",
      "====================================================================================================\n",
      "[ep:0010/200] lr=0.000390344  train_loss=6.1114  valid_loss=5.9866\n",
      "\n",
      "[예시 문장 테스트]\n",
      "정상적으로 안돌아가 싶다    -> 사랑은 항상죠\n",
      "\n",
      "너무 힘든데 어떻게 하지    -> 그게 사람을 주세요\n",
      "\n",
      "가스불을 켜놓고 나온거 같아  -> 그게 항상죠\n",
      "\n",
      "====================================================================================================\n",
      "[ep:0011/200] lr=0.000429378  train_loss=5.9564  valid_loss=5.8583\n",
      "====================================================================================================\n",
      "[ep:0012/200] lr=0.000468412  train_loss=5.8112  valid_loss=5.7682\n",
      "====================================================================================================\n",
      "[ep:0013/200] lr=0.000507447  train_loss=5.6768  valid_loss=5.6757\n",
      "====================================================================================================\n",
      "[ep:0014/200] lr=0.000546481  train_loss=5.5521  valid_loss=5.5790\n",
      "====================================================================================================\n",
      "[ep:0015/200] lr=0.000585515  train_loss=5.4239  valid_loss=5.4943\n",
      "====================================================================================================\n",
      "[ep:0016/200] lr=0.00062455  train_loss=5.2909  valid_loss=5.4233\n",
      "====================================================================================================\n",
      "[ep:0017/200] lr=0.000663584  train_loss=5.1677  valid_loss=5.3547\n",
      "====================================================================================================\n",
      "[ep:0018/200] lr=0.000702619  train_loss=5.0472  valid_loss=5.2912\n",
      "====================================================================================================\n",
      "[ep:0019/200] lr=0.000741653  train_loss=4.9343  valid_loss=5.2362\n",
      "====================================================================================================\n",
      "[ep:0020/200] lr=0.000780687  train_loss=4.8235  valid_loss=5.1893\n",
      "\n",
      "[예시 문장 테스트]\n",
      "정상적으로 안돌아가 싶다    -> 항상 못한 것 같아요\n",
      "\n",
      "너무 힘든데 어떻게 하지    -> 그게 제일 고민이죠\n",
      "\n",
      "가스불을 켜놓고 나온거 같아  -> 그게 항상 주세요\n",
      "\n",
      "====================================================================================================\n",
      "[ep:0021/200] lr=0.000819722  train_loss=4.7141  valid_loss=5.1516\n",
      "====================================================================================================\n",
      "[ep:0022/200] lr=0.000858756  train_loss=4.6046  valid_loss=5.0816\n",
      "====================================================================================================\n",
      "[ep:0023/200] lr=0.00089779  train_loss=4.5077  valid_loss=5.0634\n",
      "====================================================================================================\n",
      "[ep:0024/200] lr=0.000936825  train_loss=4.4013  valid_loss=5.0198\n",
      "====================================================================================================\n",
      "[ep:0025/200] lr=0.000975859  train_loss=4.3205  valid_loss=4.9809\n",
      "====================================================================================================\n",
      "[ep:0026/200] lr=0.000975135  train_loss=4.2318  valid_loss=4.9444\n",
      "====================================================================================================\n",
      "[ep:0027/200] lr=0.000956907  train_loss=4.1386  valid_loss=4.9077\n",
      "====================================================================================================\n",
      "[ep:0028/200] lr=0.000939664  train_loss=4.0450  valid_loss=4.8877\n",
      "====================================================================================================\n",
      "[ep:0029/200] lr=0.00092332  train_loss=3.9586  valid_loss=4.8650\n",
      "====================================================================================================\n",
      "[ep:0030/200] lr=0.000907801  train_loss=3.8842  valid_loss=4.8593\n",
      "\n",
      "[예시 문장 테스트]\n",
      "정상적으로 안돌아가 싶다    -> 지금도 늦지 않았어요\n",
      "\n",
      "너무 힘든데 어떻게 하지    -> 지금은 힘들겠지만 잘 이겨낼 수 있을 거예요\n",
      "\n",
      "가스불을 켜놓고 나온거 같아  -> 저도 해보고 싶네요예요\n",
      "\n",
      "====================================================================================================\n",
      "[ep:0031/200] lr=0.000893039  train_loss=3.8073  valid_loss=4.8402\n",
      "====================================================================================================\n",
      "[ep:0032/200] lr=0.000878975  train_loss=3.7369  valid_loss=4.8305\n",
      "====================================================================================================\n",
      "[ep:0033/200] lr=0.000865555  train_loss=3.6792  valid_loss=4.8113\n",
      "====================================================================================================\n",
      "[ep:0034/200] lr=0.000852731  train_loss=3.6175  valid_loss=4.7932\n",
      "====================================================================================================\n",
      "[ep:0035/200] lr=0.000840461  train_loss=3.5583  valid_loss=4.8070\n",
      "====================================================================================================\n",
      "[ep:0036/200] lr=0.000828706  train_loss=3.5124  valid_loss=4.8053\n",
      "====================================================================================================\n",
      "[ep:0037/200] lr=0.00081743  train_loss=3.4619  valid_loss=4.7751\n",
      "====================================================================================================\n",
      "[ep:0038/200] lr=0.000806603  train_loss=3.4190  valid_loss=4.7756\n",
      "====================================================================================================\n",
      "[ep:0039/200] lr=0.000796194  train_loss=3.3693  valid_loss=4.7726\n",
      "====================================================================================================\n",
      "[ep:0040/200] lr=0.000786179  train_loss=3.3208  valid_loss=4.7794\n",
      "\n",
      "[예시 문장 테스트]\n",
      "정상적으로 안돌아가 싶다    -> 지금이라도 진심을 전하세요\n",
      "\n",
      "너무 힘든데 어떻게 하지    -> 그 사람을 위해 에너지를 쓰니까요\n",
      "\n",
      "가스불을 켜놓고 나온거 같아  -> 좀 더 항상 주세요\n",
      "\n",
      "====================================================================================================\n",
      "[ep:0041/200] lr=0.000776532  train_loss=3.2801  valid_loss=4.7838\n",
      "====================================================================================================\n",
      "[ep:0042/200] lr=0.000767232  train_loss=3.2525  valid_loss=4.7533\n",
      "====================================================================================================\n",
      "[ep:0043/200] lr=0.000758258  train_loss=3.2088  valid_loss=4.7583\n",
      "====================================================================================================\n",
      "[ep:0044/200] lr=0.000749592  train_loss=3.1638  valid_loss=4.7413\n",
      "====================================================================================================\n",
      "[ep:0045/200] lr=0.000741217  train_loss=3.1389  valid_loss=4.7532\n",
      "====================================================================================================\n",
      "[ep:0046/200] lr=0.000733116  train_loss=3.1051  valid_loss=4.7466\n",
      "====================================================================================================\n",
      "[ep:0047/200] lr=0.000725275  train_loss=3.0733  valid_loss=4.7480\n",
      "====================================================================================================\n",
      "[ep:0048/200] lr=0.00071768  train_loss=3.0446  valid_loss=4.7346\n",
      "====================================================================================================\n",
      "[ep:0049/200] lr=0.000710319  train_loss=3.0221  valid_loss=4.7406\n",
      "====================================================================================================\n",
      "[ep:0050/200] lr=0.00070318  train_loss=2.9845  valid_loss=4.7281\n",
      "\n",
      "[예시 문장 테스트]\n",
      "정상적으로 안돌아가 싶다    -> 항상 못해본 건 궁금하더라고요\n",
      "\n",
      "너무 힘든데 어떻게 하지    -> 사랑은 유지하는 게 중요한데 대단하네요\n",
      "\n",
      "가스불을 켜놓고 나온거 같아  -> 빨리 집에 돌아가서 끄고 나오세요\n",
      "\n",
      "====================================================================================================\n",
      "[ep:0051/200] lr=0.000696252  train_loss=2.9645  valid_loss=4.7236\n",
      "====================================================================================================\n",
      "[ep:0052/200] lr=0.000689525  train_loss=2.9340  valid_loss=4.7347\n",
      "====================================================================================================\n",
      "[ep:0053/200] lr=0.000682989  train_loss=2.9056  valid_loss=4.7347\n",
      "====================================================================================================\n",
      "[ep:0054/200] lr=0.000676635  train_loss=2.8890  valid_loss=4.7415\n",
      "====================================================================================================\n",
      "[ep:0055/200] lr=0.000670456  train_loss=2.8617  valid_loss=4.7380\n",
      "====================================================================================================\n",
      "[ep:0056/200] lr=0.000664443  train_loss=2.8326  valid_loss=4.7400\n",
      "====================================================================================================\n",
      "[ep:0057/200] lr=0.000658588  train_loss=2.8148  valid_loss=4.7406\n",
      "====================================================================================================\n",
      "[ep:0058/200] lr=0.000652886  train_loss=2.7933  valid_loss=4.7443\n",
      "====================================================================================================\n",
      "[ep:0059/200] lr=0.00064733  train_loss=2.7735  valid_loss=4.7464\n",
      "====================================================================================================\n",
      "[ep:0060/200] lr=0.000641913  train_loss=2.7426  valid_loss=4.7413\n",
      "\n",
      "[예시 문장 테스트]\n",
      "정상적으로 안돌아가 싶다    -> 얼른 맛난 음식 드세요\n",
      "\n",
      "너무 힘든데 어떻게 하지    -> 사랑은 유지하는 게 중요한데 대단하네요\n",
      "\n",
      "가스불을 켜놓고 나온거 같아  -> 빨리 집에 돌아가서 끄고 나오세요\n",
      "\n",
      "====================================================================================================\n",
      "[ep:0061/200] lr=0.000636629  train_loss=2.7282  valid_loss=4.7431\n",
      "[EARLY STOP] 성능이 10번 동안 좋아지지 않아 학습을 조기 종료합니다. 가장 좋은 valid_loss=4.7236\n"
     ]
    }
   ],
   "source": [
    "# 실제로 학습을 진행할 때 사용하는 옵티마이저(optimizer)와 학습률 조정기(scheduler)를 설정합니다.\n",
    "# 옵티마이저는 모델이 더 나은 결과를 내도록 모델의 가중치(파라미터)를 조금씩 조정해주는 역할입니다.\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0,                    # 시작 학습률은 NoamOpt가 관리하므로 0.0으로 둡니다.\n",
    "    weight_decay=weight_decay,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9\n",
    ")\n",
    "\n",
    "# NoamOpt는 학습을 진행하면서 자동으로 학습률을 조절하는 역할을 합니다.\n",
    "sched = NoamOpt(\n",
    "    optimizer,\n",
    "    d_model=256,               # 단어 벡터의 차원(d_model)입니다.\n",
    "    warmup_steps=warmup_steps,\n",
    "    factor=base_lr,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# best_val은 가장 좋은 검증(validation) 손실 값을 저장하는 변수입니다.\n",
    "best_val = float(\"inf\")\n",
    "no_improve = 0   # 성능이 좋아지지 않은 횟수를 저장합니다.\n",
    "\n",
    "\n",
    "# epochs 수만큼 반복하여 학습을 진행합니다.\n",
    "for ep in range(epochs):\n",
    "\n",
    "    # 학습 데이터를 사용해 모델을 한 번 학습시키고 손실을 계산합니다.\n",
    "    train_loss = run_epoch(\n",
    "        model, train_loader, optimizer=sched,\n",
    "        label_smoothing=label_smoothing,\n",
    "        src_token_dropout=src_token_dropout\n",
    "    )\n",
    "\n",
    "    # 검증 데이터를 사용해 모델의 성능을 평가하고 손실을 계산합니다.\n",
    "    val_loss = run_epoch(\n",
    "        model, valid_loader, optimizer=None,\n",
    "        label_smoothing=label_smoothing,\n",
    "        src_token_dropout=0.0\n",
    "    )\n",
    "\n",
    "    # 학습과 검증 손실을 출력하여 학습 상태를 확인합니다.\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"[ep:{ep+1:04d}/{epochs}] lr={sched._rate:.6g}  train_loss={train_loss:.4f}  valid_loss={val_loss:.4f}\")\n",
    "\n",
    "    # 매 10번의 학습마다 실제 문장을 넣어보고 모델이 어떤 답변을 만드는지 직접 확인합니다.\n",
    "    if (ep + 1) % 10 == 0:\n",
    "        tests = [\n",
    "            \"정상적으로 안돌아가 싶다\",\n",
    "            \"너무 힘든데 어떻게 하지\",\n",
    "            \"가스불을 켜놓고 나온거 같아\",\n",
    "        ]\n",
    "\n",
    "        print(\"\\n[예시 문장 테스트]\")\n",
    "        for t in tests:\n",
    "            print(f\"{t:16} -> {reply(model, t)}\\n\")\n",
    "\n",
    "    # 검증 손실이 이전보다 좋아지면 모델을 저장하고, no_improve를 초기화합니다.\n",
    "    if val_loss < best_val - 1e-4:\n",
    "        best_val = val_loss\n",
    "        no_improve = 0\n",
    "\n",
    "        # 모델의 가중치와 설정들을 파일로 저장합니다.\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'spm_model': SPM_MODEL,\n",
    "            'vocab_size': vocab.size,\n",
    "            'config': {\n",
    "                'd_model': 256,\n",
    "                'enc_layers': 3,\n",
    "                'dec_layers': 3,\n",
    "                'dropout': 0.2,\n",
    "                'SPM_VOCAB_SIZE': SPM_VOCAB_SIZE,\n",
    "                'arch': 'lstm_seq2seq_noamfit'\n",
    "            }\n",
    "        }, save_path)\n",
    "\n",
    "    else:\n",
    "        # 좋아지지 않았으면 증가합니다.\n",
    "        no_improve += 1\n",
    "\n",
    "        # 좋아지지 않은 횟수가 early_stop_patience를 넘으면 학습을 일찍 종료합니다.\n",
    "        if no_improve >= early_stop_patience:\n",
    "            print(\n",
    "                f\"[EARLY STOP] 성능이 {early_stop_patience}번 동안 좋아지지 않아 학습을 조기 종료합니다. \"\n",
    "                f\"가장 좋은 valid_loss={best_val:.4f}\"\n",
    "            )\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acc6a694-529e-4869-930b-ba84db2f5608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[샘플 테스트]\n",
      "\n",
      "Q: 3박4일 놀러가고 싶다\n",
      "A: 여행은 언제나 좋죠\n",
      "\n",
      "Q: SD카드 안돼\n",
      "A: 다시 새로 사는 게 마음 편해요\n",
      "\n",
      "Q: 가스를 켜놓고 나온거 같아\n",
      "A: 빨리 집에 돌아가서 끄고 나오세요\n",
      "\n",
      "Q: 지망 학교 떨어졌어\n",
      "A: 즐거운 시간이 될 거 같아요\n",
      "\n",
      "Q: SNS보면 나만 빼고 다 행복해보여\n",
      "A: 술 안 마셔도 놀 수 있어요\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 아래의 tests 리스트는 인공지능 모델이 실제로 얼마나 잘 동작하는지\n",
    "# 확인하기 위해 만든 예시 질문 문장들입니다.\n",
    "# 인공지능 모델에게 이 문장들을 입력하면,\n",
    "# 모델은 각 질문에 맞는 적절한 답변을 생성합니다.\n",
    "tests = [\n",
    "    \"3박4일 놀러가고 싶다\",\n",
    "    \"SD카드 안돼\",\n",
    "    \"가스를 켜놓고 나온거 같아\",\n",
    "    \"지망 학교 떨어졌어\",\n",
    "    \"SNS보면 나만 빼고 다 행복해보여\",\n",
    "]\n",
    "\n",
    "# 예를 들어, 사용자가 \"3박4일 놀러가고 싶다\"고 입력하면,\n",
    "# 인공지능 모델이 \"여행 다녀오면 기분이 좋아질 거예요!\" 같은 답변을 생성합니다.\n",
    "\n",
    "# 생성된 질문 문장으로 실제 테스트를 실행합니다.\n",
    "print(\"\\n[샘플 테스트]\\n\")\n",
    "\n",
    "# tests 리스트에 있는 각 문장을 순서대로 하나씩 모델에 입력하여 답변을 생성합니다.\n",
    "for t in tests:\n",
    "    # 사용자의 질문(Q)을 화면에 출력합니다.\n",
    "    print(f\"Q: {t}\")\n",
    "\n",
    "    # reply 함수는 인공지능 모델을 이용하여 질문 문장(t)에 대한 답변을 생성하는 함수입니다.\n",
    "    # 예를 들어 reply(model, \"SD카드 안돼\")를 실행하면\n",
    "    # \"카드가 인식이 안되나요? 다시 꽂아보세요.\"와 같은 답변이 생성됩니다.\n",
    "    answer = reply(model, t)\n",
    "\n",
    "    # 인공지능 모델이 생성한 답변(A)을 화면에 출력합니다.\n",
    "    print(f\"A: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1237d4b-7798-442c-98d8-53891f88ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:  \n",
    "    # 무한히 반복하여 사용자의 입력을 계속 기다립니다.\n",
    "    try:\n",
    "        # input 함수는 사용자가 입력한 문장을 받아오는 함수입니다.\n",
    "        # 사용자는 화면에 나타나는 > 뒤에 문장을 입력합니다.\n",
    "        u = input(\"> \").strip()\n",
    "        # 입력받은 문장에서 앞뒤 공백을 제거합니다(.strip()).\n",
    "        # 예를 들어 사용자가 \"  안녕? \"이라고 입력하면 \"안녕?\"으로 바뀝니다.\n",
    "\n",
    "    except EOFError:\n",
    "        # EOFError는 입력이 갑자기 종료될 때 발생합니다.\n",
    "        # 이 오류가 발생하면 입력이 끝난 것으로 무한반복을 종료(break)합니다.\n",
    "        break\n",
    "\n",
    "    if not u:\n",
    "        # 사용자가 아무 것도 입력하지 않고 그냥 엔터만 누르면, 빈 문자열이 입력됩니다.\n",
    "        # 이 경우에는 답변을 만들지 않고, 다시 입력을 기다립니다(continue).\n",
    "        continue\n",
    "\n",
    "    if u.lower() in (\"/quit\", \"/exit\"):\n",
    "        # 사용자가 \"/quit\" 또는 \"/exit\"를 입력하면,\n",
    "        # 대화형 테스트를 종료합니다.\n",
    "        # 예를 들어 \"/quit\"를 입력하면 프로그램이 종료됩니다.\n",
    "        break\n",
    "\n",
    "    # 사용자가 정상적으로 질문 문장을 입력한 경우(reply 함수 사용):\n",
    "    # reply 함수는 입력 문장(u)을 인공지능 모델(model)에 넣어 답변을 만들어줍니다.\n",
    "    # 예를 들어 사용자가 \"기분 우울해\"라고 입력하면,\n",
    "    # 모델이 \"힘들었겠어요. 어디 아픈 곳은 없나요?\"와 같은 답변을 생성할 수 있습니다.\n",
    "    # max_len은 생성되는 답변의 최대 길이를 80단어로 제한합니다.\n",
    "    answer = reply(model, u, max_len=80)\n",
    "\n",
    "    # 생성된 답변(answer)을 화면에 출력합니다.\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c13ee-5fbc-44b1-b58f-1eafd1649c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
